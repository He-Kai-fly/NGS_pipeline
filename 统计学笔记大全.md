统计学

**by: 何凯**

**“Talk is cheap, show me the code.”——空谈无益，秀代码。**

[toc]

## 知识点



### 常见的分布形式

![image-20220423114330627](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202204231143823.png)

![image-20211225165142644](https://gitee.com/kai_kai_he/PicGo/raw/master/image4/image-20211225165142644.png)

[一行代码画出正态分布图！轻松计算某个数值的概率~ (qq.com)](https://mp.weixin.qq.com/s/PIK0Nmosj31K7QW0o5M7hA)

```R
#超几何检验
phyper(k-1, M, N2-M, n*1.5, lower.tail=F)

any(c(1, 2, 3, 4, 5, 6, 7) > 3) 判断是否其中有真值
all(c(1, 2, 3, 4, 5, 6, 7) > 3) 判断是否都为真值

#数据的分布
dnorm(x, mean=0, sd=1, log = FALSE) 正态分布的概率密度函数
pnorm(q, mean=0, sd=1, lower.tail = TRUE, log.p = FALSE) 正态分布的分布函数
qnorm(p, mean=0, sd=1, lower.tail = TRUE, log.p = FALSE) 给定概率p后的下分为点
rnorm(n, mean=0, sd=1) n个正态分布随机数构成的向量

分布的可视化：
直方图的制作适合于总体为连续型分布的场合.对于一般的总体分布，若要估计它的总体分布函数F（x），可用经验分布函数（empirical distribution function）
1.直方图
hist(x) 参数freq是逻辑向量 TRUE/counts绘制频率直方图，FALSE会出密度直方图 probability与freq相反
2.核密度估计函数
density(x)
hist(w, freq = FALSE)
lines(density(w), col = "blue")
lines(x, dnorm(x, mean(w), sd(w)), col = "red")
3.经验分布
其中，在函数ecdf（）中的x是由观察值得到的数值型向量，而在函数plot（）中的x是由函数ecdf（）生成的向量.verticals是逻辑量，当verticals=TRUE表示画竖线；否则（FALSE，缺省值）不画坚线.
plot(ecdf(w),verticals = TRUE, do.p = FALSE)
x <- 44:78
lines(x, pnorm(x, mean(w), sd(w)))
4.QQ图
不论是直方图还经验分布图，要从比较上鉴别样本是否近似于某种类型的分布是困难的，QQ图可以帮助我们鉴别样本的分布是否近似于某种类型的分布.
qqnorm(w); qqline(w)

#与直方图相比，茎叶图更能细致地看出数据分布的结构
stem(x)

在R软件中，函数shapiro.test（）提供W统计量和相应的p值，当p值小于某个显著性水平a（比如0.05），则认为样本为不是来自正态分布的总体；否则承认样本来自正态分布的总体.

cor.test(~X2+X3, data=rubber)做相关性检验
```

- d = 密度函数
- p = 分布函数
- q = 分位数函数
- r = 生成随机数

```
dnorm(3,0,2) #正态分布N（0,4）在3处的密度值
```

![image-20210323145314536](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20210323145314536.png)



### 标准化与中心化

[数据标准化（一） - Z-Score标准化](https://zhuanlan.zhihu.com/p/69074703)

[z-score的标准化究竟怎么弄？](https://mp.weixin.qq.com/s?__biz=MzIyMzA2MTcwMg%3D%3D&mid=2650734070&idx=1&sn=ef4cc3f5ad85b58cf4997b4c74272038&scene=45#wechat_redirect)

数据中心化和标准化在回归分析中是取消由于量纲不同、自身变异或者数值相差较大所引起的误差。

目的：通过中心化和标准化处理，得到均值为0，标准差为1的服从标准正态分布的数据。

**数据标准化（Zero-centered或者Mean-subtraction）**：是指数值减去均值，再除以标准差,也叫做Z-score标准化；准化后的数据均值为0，标准差为1（方差也为1）
**数据中心化(Zero-centered 或者Mean-subtraction)**：是指变量减去它的均值。**中心化后的数据均值为零**，其实就是一个数据平移的过程，平移后的数据中心是(0,0)

不同的问题中，中心化和标准化有着不同的意义，比如在训练神经网络的过程中，通过数据标准化能够加速权重参数的收敛，对数据进行中心化的预处理，目的是增加基向量的正交性。



![image-20220423104837318](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202204231048901.png)

**Code**

```r
#数据中心化
centralized <- function(x) {
  x - mean(x)
}
penguins %>%
  mutate(
    across(c(bill_length_mm, bill_depth_mm), centralized)
  )


## 数据标准化
std <- function(x) {
  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}

penguins %>%
  mutate(
    across(c(bill_length_mm, bill_depth_mm), std)
  )

penguins %>% 
summarise(
across(starts_with("bill_"), ~(.x-mean(.x))/sd(.x))
)
```

**归一化**

归一化后加速梯度下降求最优解的速度，也可以提高精度

1. min-max标准化 也称为离差标准循环，是对原始数据的线性转变，使结果映射到0-1之间，缺陷是当有新数据加入时候，可能导致max-min的变化，需要重新定义
2. Z-score标准化：这种方法给予原始数据的均值和标准差进行数据的标准化
3. 平均归一化 x' = (x - μ) / (MaxValue - MinValue)
4. 非线性归一化
     1）对数函数转换：y = log10(x)
        2）反余切函数转换：y = atan(x) * 2 / π

```R
# min-max 标准化（Min-Max Normalization）
x <- matrix(c(1,2,3,4,5,6), ncol=2)
y <- scale(x, center = apply(x, 2, min), 
           scale = apply(x,2,function(x) {max(x)-min(x)})
           )


# Zscore标准化
mat_scale <- round(t(apply(mat, 1, scale)),2)

data_A <- rnorm(100, 80, 10) # randomly create population dataset

data_B <- rnorm(100, 400, 100) # randomly create population dataset
hist(data_A) #histogram
hist(data_B) #histogram


#Calculate population mean and standard deviation
A_data_std <- sd(data_A)*sqrt((length(data_A)-1)/(length(data_A)))
A_data_mean <- mean(data_A)


B_data_std <- sd(data_B)*sqrt((length(data_B)-1)/(length(data_B)))
B_data_mean <- mean(data_B)


# Provided that A got 92 and B got 610
A_obs <- 92
B_obs <- 610

A_Z_score <- (A_obs - A_data_mean) / A_data_std
B_Z_score <- (B_obs - B_data_mean) / B_data_std
```



### 卡方检验

[(7条消息) 一文详解 如何用 R 语言进行卡方检验。_小风alter的博客-CSDN博客_r语言做卡方检验](https://blog.csdn.net/weixin_44298740/article/details/107225872)





### 差异检验

[组间差异分析简介](https://www.jianshu.com/p/67be9b3806cd)

[差异分析完美解决方案Easystat](https://mp.weixin.qq.com/s/5C_JFvTaKcefueSpwxzGQQ)

[三万字懂得微生物群落差异分析（上）](https://mp.weixin.qq.com/s/8wSRIurTyDAjRrmpIJH8JA)

[墙裂推荐！统计方法如何选以及全代码作图实现。](https://mp.weixin.qq.com/s/IF4F0W2ghWRq4ILVP3T49A)

[非参数检验的全能R包：PMCMRplus (qq.com)](https://mp.weixin.qq.com/s/zUHbhvBlt2cRWOr-A9qz0g)

![image-20210718113539365](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20210718113539365.png)

**补充说明：**

1. 对数据进行统计分析前，务必了解清楚分析方法适用的前提假设条件

2. 经 ANOVA（或 Kruskal-Wallis test）检验差异有统计学意义（alpha = 0.05），需要对每两个均数进行比较，需要采用上图所述“两两比较方法”，而**不能直接对每两组数据进行t-test（或 Mann-Whitney U-test），因为会增加犯 I 类错误 的概率**：

   例如三组数据资料，ANOVA结果显示 *p* < 0.05；**然后每两组均数t-test比较一次，则需比较3次，那么比较3次至少有一次犯 I 类错误 的概率就是 alpha' = 1-0.95^3 = 0.1426 > 0.05**。

   ![image-20211225170619865](https://gitee.com/kai_kai_he/PicGo/raw/master/image4/image-20211225170619865.png)

   4.对于双样本t-test讨论：

   z-test：大样本；>30;z分布；

   t-test：小样本;<30;t分布；

   但是，**对于 > 30 的样本**，Z-test检验要求知道总体参数的标准差，在理论上成立，事实上总体参数的标准差未知，**实际应用中一般使用t-test**.



```R
library(multcomp)
#加载数据
data("cholesterol") 
head(cholesterol)

#正态分布检验：用shapiro.test()
shapiro.test(cholesterol$response)    

#方差齐性检验：用bartlett.test()或者leveneTest()
bartlett.test(response~trt,data = cholesterol)   #巴雷特检验

library(car)  #leveneTest()属于car包
leveneTest(cholesterol$response~cholesterol$trt)  #列文检验
```



如果是非配对的T检验，那就选择不需要加上两组方差相等的检验

进行T-test检验时要注意：是双尾检验（存在差异）还是单尾检验（显著性上调或下降），一般检验使用双尾T检验更为保守。两个样本的总体是不是等方差（标准T检验还是Welch’s test）如果存在多于两个处理（条件），就需要用到ANOVA分析了。ANOVA分析能主要是研究结果之间的差异是如何引
起的.
对于基因芯片的数据而言，由于样本服从正态分布，所以可以用t-test（双处理）或anova分析（多处理以上）。现在的RNA-Seq，它的抽样过程是离散的，结果是count，服从泊松分布，样本间的差异是服从负二向分布，显然不能按照上述方法分析。

方差分析（ANOVA)和线性回归分析(regression)都是同一时期发展的两套紧密相连的理论。方差分析考量的是离散型自变量(因子）对连续型应变量（响应变量）的模型分析，而线性回归分析只要求响应变量是连续的，对于自变量无要求。如果响应变量不是连续型分布，就要使用更加一般化的广义线性模型（generalized linear model),通过一个连接函数变换响应变量期望，将响应变量的期望与自变量建立线性关系。

推荐在统计检验前过滤表达量低，也就如果一个基因在所有样本中count均低于某一阀值，请在分析前剔除。这个阀值也是约定俗成，一般设置为3。

填补缺失数据最简便的方法使用一些代表中心趋势的值，反应变量分布的最常见值，正态分布，平均值是最佳选择，偏态分布或者有离群值的变量来说，中位数最佳

常见的连续性分布：均匀分布  指数分布 正态分布

常见的离散性分布：两点分布（0-1分布） Bernoulli实验，二项分布 ；Poisson分布

![image-20200417094719934](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20200417094719934.png)

#### QQ图

[超完整！QQ图绘制方法大汇总~~](https://mp.weixin.qq.com/s/xdoNFA6CZLWAb7CLCn7SRw)

```R
library(ggpubr)
x <- rnorm(250 , mean=10 , sd=1)
ggqqplot(x,
         shape=21,fill="white",colour="black", #使用白色填充、黑色边框的圆圈绘制数据点
         ggtheme = ggplot2::theme_grey()) #采用ggplot2 包的灰色风格
```











#### 方差齐性检验

[方差齐性检验的三种方法](https://mp.weixin.qq.com/s?src=11&timestamp=1626086499&ver=3186&signature=YSBDGrK9OUi*9AM4FteGcSZw8d4AZxZ0mrVge4NcdX5yXwA1jeax11OpugcCBTWVQnsd-kg1BNCbbcD0zyzMjLgSfAU1Nu*q82KJsQDWjAfaH6PQ25xlmPiAMjlqE-CX&new=1)

```R
high <- c(134, 146, 104, 119, 124, 161, 107, 83, 113, 129, 97, 123)
low <- c(70, 118, 101, 85, 107, 132, 94)

#方差齐次性检验
x <- c(134, 146, 104, 119, 124, 161, 107, 83, 113, 129, 97, 123,70, 118, 101, 85, 107, 132, 94)
a <- factor(c(rep(1,12),rep(2,7)))
#bartlett.test方差齐性检验
bartlett.test(x~a)
#Bartlett检验结果表明：p-value=0.002798<0.05，可以认为方差不齐。
#var.test方差齐性检验
var.test(x~a)
#levene.test方差齐性检验（也是SPSS的默认方差齐性检验方法）
library(car)
levene.test(x~a)
#前两者是对原始数据的方差进行检验的，leveneTest是对方差模型的残差进行组间齐性检验.一般认为是要求残差的方差齐，所以一般的统计软件都做的是leveneTest

#t检验
t.test(high,low,paired = FALSE)
1.方差齐次性检验，取var.test方差齐性检验的结果，F = 1.0755,p-value = 0.9788>0.05，说明两独立样本数据方差齐性

2.我们关注的是上表中方差“等于”对应的t值，t=1.89，p值0.0757>0.05，不拒绝原假设，不能认为两组雌鼠体重增加量不相等
```



[R语言执行两组间差异分析T检验](https://mp.weixin.qq.com/s?__biz=MzIxNzc1Mzk3NQ==&mid=2247483879&idx=1&sn=62c4f96a9fb331363304678bbb5dd0cd&chksm=97f5b1ffa08238e9873944636d225b723e850b3dc7552caf8da06866c5c911c82f352af71584&token=66128012&lang=zh_CN&scene=21#wechat_redirect)

```R
#QQ图 检验数据的正太性 是否符合正太分布模型
qqPlot(lm(value~group, data = shannon), simulate = TRUE, main = 'QQ Plot', labels = FALSE)
#Shapiro-Wilk 检验   验证数据分布的正太性
#通常情况下若通过Shapiro-Wilk检验得到p值小于0.05，则拒绝原假设，数据分布不符合正态性；反之接受原假设。
shapiro <- tapply(shannon$value, shannon$group, shapiro.test)
shapiro$'D'$p.value
shapiro$'H'$p.value
#无论Bartlett检验还是Levene检验，两者的P值都大于0.05，#因此接受原假设：样本之间的方差是相同的。因此可以接着做方差分析了。
##Shapiro-Wilk 检验，当且仅当两者 p 值均大于 0.05 时表明数据符合正态分布

# Bartlett检验
bartlett.test(shannon$value ~ shannon$group, data=shannon)
# Levene检验,对原始数据的正态性不敏感
leveneTest(shannon$value ~ shannon$group, data=shannon)
#Shapiro-Wilk 检验批量验证
alpha$group <- factor(alpha$group)
alpha_all <- levels(alpha$variable)
group_all <- levels(alpha$group)
result <- NULL
for (a in alpha_all) {
  for (i in 1:(length(group_all) - 1)) {
    for (j in (i + 1):length(group_all)) {
      dat <- subset(alpha, variable == a & group %in% c(group_all[i], group_all[j]))
      shapiro <- tapply(dat$value, dat$group, shapiro.test)
      p1 <- shapiro[[group_all[i]]]$p.value
      p2 <- shapiro[[group_all[j]]]$p.value
      ifelse(p1 < 0.05 | p2 < 0.05, pass <- 'no', pass <- 'yes')
      result <- rbind(result, c(a, group_all[i], p1, group_all[j], p2, pass))
    }
  }
}
result <- data.frame(result)
names(result) <- c('alpha', 'group1', 'p.value1', 'group2', 'p.value2', 'pass')
write.table(result, '11.txt', sep = '\t', row.names = FALSE, quote = FALSE)
```

#### T/Wilcox检验

[R：使用t.test函数对多列进行t测试](https://www.it1352.com/1581321.html)

[R语言t检验_Tiaaaaa的博客-CSDN博客_r语言t检验](https://blog.csdn.net/tiaaaaa/article/details/58130363)

[【R语言】三种批量做T检验的方法 (qq.com)](https://mp.weixin.qq.com/s/LHTZU6H075UEreJ5KEtsCA)

```R

#符合上述则接着进行t检验
#独立样本t检验 
#t.test不假设有方差齐性（或称作方差同质）。默认的不是Student t检验而是使用了Welch t检验(方差不相等)。要使用Student t检验的话，设置var.equal=TRUE。
t_test <- t.test(value~group, shannon, paired = FALSE)
t_test$p.value
#非独立样本t检验（配对T）
t_test <- t.test(value~group, shannon, paired = TRUE)
t_test$p.value
#boxplot箱线图
boxplot(value~group, data = chao, col = c('grey', 'white'), ylab = 'Shannon', xlab = 'Group', main = 't-test: p-value <0.01')#非独立样本t检验
#ggplot2 柱形图
library(doBy)	#使用其中的 summaryBy() 以方便按分组计算   scale_y_continuous(expand = c(0, 0))可将图对齐至坐标轴
library(ggplot2)	#ggplot2 作图
dat <- summaryBy(value~group, coverge, FUN = c(mean, sd))
#箱线图
p <- ggplot(Simpson,aes(x=group,y=value,fill=group))+
  stat_boxplot(geom = "errorbar",width=0.15)+geom_boxplot(outlier.colour = NA)+
  scale_fill_manual(values = c("red", "blue"))+#notch=F缺口
  xlab("")+ylab("Simpson index")+#legend.title = element_blank()
  theme(panel.grid = element_blank(), panel.background = element_rect(fill = 'transparent', color = 'black'), legend.title = element_blank(),legend.key = element_blank())
p
ggsave(plot = p,"Simpson_boxplot1.pdf",height=5,width=6,dpi=300)
#误差棒的柱状图
ggplot(dat, aes(group, value.mean, fill = group))+
  geom_col(width = 0.4, show.legend = FALSE) +
  geom_errorbar(aes(ymin = value.mean - value.sd, ymax = value.mean + value.sd), width = 0.15, size = 0.5) +
  theme(panel.grid = element_blank(), panel.background = element_rect(color = 'black', fill = 'transparent'), plot.title = element_text(hjust = 0.5)) +
  labs(x = 'Group', y = 'Shannon', title = 't-test: p-value < 0.01')+scale_y_continuous(expand = c(0, 0))+theme_classic()

***********************************
  
#非参数检验之wilcox秩和检验（或称Mann-Whitney U检验）
shannon_23 <- subset(alpha, variable == 'Shannon' & group %in% c('D', 'H'))
shannon_23$group <- factor(shannon_23$group)
#Shapiro-Wilk 检验数据是否符合正态分布
tapply(shannon_23$value, shannon_23$group, shapiro.test)

#Wilcox秩和检验（独立T检验的一种替代的非参检验）
#wilcox 秩和检验
wilcox_test <- wilcox.test(value~group, chao)#默认paired = FALSE 默认参数alternative = 'two.sided'
#可分别使用“alternative = 'less'”或“alternative = 'greater'”执行单侧wilcox检验。
wilcox_test
wilcox_test$p.value

#wilcox符号秩和检验（非参非独立T检验的一种替代非参检验）
wilcox_test <- wilcox.test(value~group, shannon_23, paired = TRUE)
wilcox_test
wilcox_test$p.value
rm(list = ls())

*****************
  #批处理 Wilcox秩和检验
  ##wilcox 检验批处理示例
  library(doBy)  #使用其中的 summaryBy() 以方便按分组计算均值、中位数
setwd('E:\\Study\\Bioinformation\\Script\\Wilcox')
gene <- read.table('OTU_table.txt', sep = '\t', row.names = 1, header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
group <- read.table('sample.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
result <- NULL

#计算P值只显示P<0.05的值
for (n in 1:nrow(gene)) {
  gene_n <- data.frame(t(gene[n,subset(group, group %in% c("D","H"))$sample]))
  gene_id <- names(gene_n)[1]
  names(gene_n)[1] <- 'gene'
  gene_n$sample <- rownames(gene_n)
  gene_n <- merge(gene_n, group, by = 'sample', all.x = TRUE)
  gene_n$group <- factor(gene_n$group)
  p_value <- wilcox.test(gene~group, gene_n)$p.value#加上exact=FALSE可避免Warning
  if (!is.na(p_value) & p_value < 0.05) {
    stat <- summaryBy(gene~group, gene_n, FUN = c(mean, sd))
    result <- rbind(result, c(gene_id, as.character(stat[1,1]), stat[1,2], stat[1,3], as.character(stat[2,1]), stat[2,2], stat[2,3], p_value))
  }
}

result <- data.frame(result)
names(result) <- c('gene_id', 'group1', 'mean1', 'median1', 'group2', 'mean2', 'median2', 'p_value')
write.table(result, 'sd.txt', sep = '\t', row.names = FALSE, quote = FALSE)
```

#### Hotelling's t-squaed test 

[Hotelling's t-squared test及在R语言中的计算 (qq.com)](https://mp.weixin.qq.com/s?__biz=MzIxNzc1Mzk3NQ==&mid=2247484798&idx=1&sn=7fafc84d2646187eccba126d617639a1&chksm=97f5b566a0823c700e214da5071fa7e7c46b94514c0f8f5352c3fc9ceef2a87f68f6b3e21019&scene=178&cur_album_id=1366886129563254785#rd)

是T检验的的多元形式，要求数据：

（1）变量服从正态分布，在这儿表现为多元正态性；

（2）变量间具有相等的方差-协方差矩阵，即方差-协方差矩阵同质性。

Hotelling's t-squared test只能用于两组样本的比较，对于两组以上样本，考虑执行MANOVA多元方差分析。





#### cohen‘d检验

[效应量的计算——Cohen's d statistic - 云+社区 - 腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/1634971)

如皮尔森相关的r值，r = 0.1，0.3，0.5分别被定义为small,medium,large的效应量。 本文主要关注的Cohen's d，d = 0.01,0.2,0.5,0.8,1.2,2.0分别被定义为very small,small,medium,large,very large和huge的效应量。

#### 一致性检验

[相关性/一致性检验一站式R教程 (qq.com)](https://mp.weixin.qq.com/s/DOq12I3co1noImVXV0oKgQ)





#### **显著性标记俩种方法**

[P值的几种分布](https://mp.weixin.qq.com/s/t_UyEjb-bVkzqFGpj7Uv8Q)

[浅谈多重检验校正FDR        ](https://www.bioinfo-scrounger.com/archives/92/)

对于显著性水平的标注，常见两种样式，一种是“*”，另一种是“abc”。

（1）0.01≤p<0.05，标注“*”；

（2）0.001≤p<0.01，标注“**”；

（3）p<0.001，标注“***”；

无论参数型检验还是非参数型检验都通用。

参数检验，如在多重比较中，对于“abc”的标注，需要同时结合显著性水平以及均值等信息。默认做法如下：

（1）首先根据均值大小，将各组由高往低排排序，均值最高的组标注为“a”；

（2）将均值最高的组与第二高的组相比，若差异显著，则第二组标注为“b”；若不显著，继续比较其与均值第三高的组的差异；

（3）若均值最高的组与第二高的组不显著，均值第二高的组与第三高的组显著，则第二高的组就同样标注为“a”，第三高的组标注为“b”；若均值最高的组与第二高的组不显著、均值第二高的组与第三高的组不显著，但均值最高的组与第三高的组显著，则第二高的组就标注为“ab”，第三高的组标注为“b”；

（4）然后以标注为“b”的组的均值为标准，以此类推，继续循环往后比较，直到最小均值的组被标记，且比较完毕为止。

在这种模式下，只要两组间达到显著性差异水平，即为一个层级；对于显著性差异到底多大，并不是很侧重。





## 方差分析

### 事后多重比较的方法

[如何理解事后多重比较的方法？ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/49479404)

[R语言统计—多重比较（1） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/387251772)

[R语言统计—多重比较（2） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/387956936)

[浅析R语言单因素方差分析中的多重比较](https://mp.weixin.qq.com/s/KMxsLPky4VzfFYHgpDwp2Q)

在我们实际数据分析中，通常会碰到两组以上定量数据的比较，对于这种需求我们不能再进行t检验，因为这样会增加一类错误的概率。

多个样本均数比较可以使用以下方法(含R中实现)：

**Bonferroni法**

Bonferroni 法的实质是对检验水平进行调整，调整的检验水平为：α=0.05/调整的次数。Bonferr0ni 法比较保守。(校正后的P值)

```R
Usage
pairwise.t.test(x, g, p.adjust.method = p.adjust.methods,
                pool.sd = !paired, paired = FALSE,
                alternative = c("two.sided", "less", "greater"),
                ...)
         
pairwise.t.test(airquality$Ozone, airquality$Month, p.adj = "bonf",data=airquality)       
               
```

**Dunnett检验**

[How to Perform Dunnett's Test in R - Statology](https://www.statology.org/dunnetts-test-r/)

Dunnett检验适用于实验设计阶段就计划了某些均数的两两比较，最常见的就是多个处理组和实验组之间的比较。

Dunnett检验的统计量为tD,在R中==multcomp==包中的glht函数可以进行Dunnett检验。

```
kl<-airquality
kl$Month<-as.factor(kl$Month)
mod <- aov(Ozone ~Month , data = kl)
dun<-glht(mod, linfct = mcp(Month = "Dunnett"))
summary(dun)

### 批量计算
rm(list = ls())
kl<-airquality
kl$Month<-as.factor(kl$Month)
gc <- colnames(kl)[1:4]
result <- NULL
for (i in 1:length(gc)){
  bar_tmp <- kl[,c(gc[i],'Month')]
  colnames(bar_tmp) <- c('Expression','Group')
  mod <- aov(Expression ~ Group , data = bar_tmp)
  dun<-glht(mod, linfct = mcp(Group = "Dunnett")) 
  result <- rbind(result, broom::tidy(dun))
}
result$Type <- c(rep(gc,each = 4))
re <- dplyr::select(result,c(8,2,7))
re1 <- dcast(re,Type~contrast)
```



**Tukey的HSD法**

若设计了对照组，要进行 k-1 个组与某个对照组之间的比较

```
在R中进行Tukey的HSD检验可通过TukeyHSD函数实现
kl<-airquality
kl$Month<-as.factor(kl$Month)
mod <- aov(Ozone ~Month , data = kl)
TukeyHSD(mod,"Month")
```

因为Tukey的HSD检验比Bonferroni法更加的保守，Tukey的HSD法要求各样本的样本相等或者接近，在样本量相差很大的情况下还是建议使用其他方法。

**SNK法**

SNK 法最为常用，但当两两比较的次数极多时，该方法的假阳性很高，最终可以达到 100%。因此比较次数 较多时，不推荐使用；

SNK法属于多重极差检验，其检验统计量为q，又称q检验。SNK法与Bonferroni法在功能上类似，SNK法是Tukey的HSD检验法的一种修正，相对于Tukey的HSD检验不是很保守，因此能发现更多的差异。

SNK法的实现可以通过R中的agricolae包的SNK.test函数实现

```
 kl<-airquality
kl$Month<-as.factor(kl$Month)
mod <- aov(Ozone ~Month , data = kl)
library(agricolae)
SNK.test(mod,trt="Month",console= T)
```

**LSD法**

若存在明显的对照组，要进行的是“验证性研究”，即计划好的某两个或几个组间的比较，宜用 LSD 法；

最小显著差异（least significnat difference,LSD)法由统计学家Fisher提出，因此也称为Fisher的最小显著差异法，简称LSD法。它其实只是t检验的一个简单变形, 并未对检验水准做出任何校正, 只是在标准误的计算上充分利用了样本信息, 为所有组的均数统一估计出了一个更为稳健的标准误,

其中，ＬＳＤ方法能灵敏地发现差异，当ｋ不是很大时，常用 ＬＳＤ 方法．而确定相似子集常 用ＳＮＫ 方法．

适用情况：事先计划好要对某对或某几对均值进行比较，不管方差分析结果如何，都要进行比较。

在R中agricolae包中的LSD.test函数，以及DescTools包中的PostHocTest函数均可以进行LSD检验，推荐使用后者（能给P值），LSD.test函数是 用**标字母法**表示组间差异。

```R
library(DescTools)
kl<-airquality
kl$Month<-as.factor(kl$Month)
mod <- aov(Ozone ~Month , data = kl)
PostHocTest(mod,method = "lsd")
```





### 单因素（非参）

**因此当实验组数多余三组时，研究者倾向于使用Tukey氏检验**

[R中npmc的非参数比较](https://mp.weixin.qq.com/s/wYirhj8S9Cdlf1j_Bp2rAg)

```R
#对于多组数据间比较差异，我们常会想到使用方差分析（ANOVA）来实现。不过由于ANOVA的前提假设条件比较严格，要求数据必须满足正态性、方差齐性等，而很多情况下我们的数据并不符合方差分析的条件。通常情况下，我们可以考虑转换数据，例如，使用log转换等或许可以使非正态分布的原始数据转变为正态分布类型（当然，我们需要确保转换后的数据能够被合理解释，否则将无意义）；另一种合适的选择是使用非参数的检验方法，替代ANOVA。，尽可能使用ANOVA分析，能够有效地鉴别出非参数检验鉴别不到的差异；当无法适用ANOVA时，再考虑非参数的方法。
#读入文件
soil <- read.table('soil.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
group  <- read.table('group.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
soil <- merge(soil, group, by = 'sample')

##单因素方差分析的非参数检验替代方法
#假设样本采自三种土壤环境（A、B、C），我们比较三种土壤环境下的细菌群落的 chao1 指数是否存在显著差异

#以 chao1 指数为例，同时将分组列转换为因子变量
chao1 <- soil[ ,c('sample', 'site', 'chao1')]
chao1$site <- factor(chao1$site)

#当数据不满足单因素方差分析的条件（正态性、方差齐性）时，尝试非参数的方法，例如
#Kruskal-Wallis Test
kruskal.test(chao1~site, data = chao1)

#若想查看各组中位数、极大（小）值、（四）分位数，可使用 aggregate()
aggregate(chao1$chao1, by = list(chao1$site), FUN = max)
aggregate(chao1$chao1, by = list(chao1$site), FUN = function(x) quantile(x, 0.75))
aggregate(chao1$chao1, by = list(chao1$site), FUN = median)
aggregate(chao1$chao1, by = list(chao1$site), FUN = function(x) quantile(x, 0.25))
aggregate(chao1$chao1, by = list(chao1$site), FUN = min)

#如果各组不独立（如重复测量设计或随机区组设计），那么 Friedman Test 可能会更合适
#这里以 ?friedman.test 中的示例为例
wb <- aggregate(warpbreaks$breaks, by = list(w = warpbreaks$wool, t = warpbreaks$tension), FUN = mean)
friedman.test(x ~ w | t, data = wb)

##此时若想继续探寻两两分组间的差异，可使用 Wilcoxon 秩和检验，如果分组较多，可以使用循环来完成
#继续在上述 Kruskal-Wallis 检验的基础上，探究两两分组间差异，一个示例如下
group <- levels(chao1$site)
group1 <- NULL
group2 <- NULL
median1 <- NULL
median2 <- NULL
p <- NULL

for (i in 1:(length(group) - 1)) {
	for (j in (i + 1):length(group)) {
		group1 <- c(group1, group[i])
		group2 <- c(group2, group[j])
		group_ij <- subset(chao1, site %in% c(group[i], group[j]))
		group_ij$site <- factor(group_ij$site, levels = c(group[i], group[j]))
		
		wilcox_test <- wilcox.test(chao1~site, data = group_ij, alternative = 'two.sided', conf.level = 0.95)
		p <- c(p, wilcox_test$p.value)
		median1 <- c(median1, median(subset(group_ij, site == group[i])$chao1))
		median2 <- c(median2, median(subset(group_ij, site == group[j])$chao1))
	}
}

result <- data.frame(group1, group2, median1, median2, p)
result$padj <- p.adjust(result$p, method = 'holm')	#推荐加上 p 值校正，这里使用 Holm 方法校正 p 值
result
#write.table(result, 'Wilcoxon.txt', sep = '\t', row.names = FALSE, quote = FALSE)

#使用 ggpubr（ggplot2 的扩展包），作图展示 wilcox 比较的结果
library(ggpubr)

ggboxplot(data = chao1, x = 'site', y = 'chao1', color = 'site') +
stat_compare_means(method = 'wilcox.test', comparisons = list(c('A', 'B'), c('A', 'C'), c('B', 'C')))
    
##与Kruskal-Wallis检验相比，如果各组不独立（如重复测量设计或随机区组设计），那么Friedman检验可能会更合适。尽管此时你仍然使用Kruskal-Wallis检验也可以。
#使用 ?friedman.test 查看帮助文档，这里以帮助文档中最下方提供的示例为例
wb <- aggregate(warpbreaks$breaks, by = list(w = warpbreaks$wool, t = warpbreaks$tension), FUN = mean)
friedman.test(x ~ w | t, data = wb)
```

**此外另一种常见做法则是使用非参数的多重比较. R包，npmc，提供了一种非参数多组比较程序npmc()，可在控制I类错误的前提下，进行所有组之间的成对比较。。**

```R
library(npmc)
data("brain")
head(brain)
#Bartlett 检验显示 p 值大于 0.05，即拒绝方差齐性假设
bartlett.test(var~class, data = brain)
#但由于原始数据不满足方差齐性，无法通过单因素ANOVA解决，所以考虑使用非参数的方法。
#Kruskal-Wallis 检验，整体差异
fit <- kruskal.test(var~class,data = brain)
fit

#npmc()函数接受的输入为一个两列的数据框，其中一列名为class（分组变量），另一列名为var（因变量）。因此在输入数据前，更改变量名称保证能够被npmc()识别。
#npmc()函数中提供了两种非参数多重比较方法，Behrens-Fisher和Steel，均可用于成对和多对一的比较情况，并同时计算相对效应的置信区间（1-alpha）。
#npmc 的非参数多重比较
npmc_test <- npmc(brain, df = 2, alpha = 0.05)

#概要
summary(npmc_test, type = 'both', short = FALSE)
#或者
npmc_test$test
#单侧检验p值，与双侧检验相比它只考虑一端情况，若a-b比较组中a和b存在显著差异则代表a显著小于b；一般情况下，关注双侧检验的结果就可以了。
#ggplot2 箱线图
library(ggplot2)

p <- ggplot(data = brain, aes(x = class, y = var, fill = class)) +
  geom_boxplot(outlier.size = 1) +
  theme(panel.grid = element_blank(), panel.background = element_rect(fill = 'transparent', color = 'black'),
        legend.title = element_blank(), legend.key = element_blank(), plot.title = element_text(hjust = 0.5)) +
  labs(x = '', y = '', title = 'Behrens-Fisher\n')

p +
  annotate('segment', x = 1, xend = 2, y = 50, yend = 50) +
  annotate('segment', x = 1, xend = 3, y = 55, yend = 55) +
  annotate('segment', x = 2, xend = 3, y = 60, yend = 60) +
  annotate('text', x = 1.5, y = 52, label = 'p = 0.147') +
  annotate('text', x = 2, y = 57, label = 'p = 0.754') +
  annotate('text', x = 2.5, y = 62, label = 'p = 0.044')
```

### 单因素（有参）

[跟着Nature学作图 | 质控箱线图 (qq.com)](https://mp.weixin.qq.com/s/4Js4BczzLTCesb45fndePA)

![](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202204021120012.png)



![image-20210421155449293](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20210421155449293.png)

```R
#对于两组数据间的差异分析，最常见的方法就是使用T检验比较两组均值是否存在显著不同。当拓展到多组（三组及以上）时，使用T检验逐一两两比较的方法无疑是低效的，不仅仅由于需要的检验次数增多，而且发生I型错误（拒绝真）的概率也会增大。Fisher提出一种广义T检验的方法来比较三组及以上总体的均值，称为方差分析（ANOVA）。
#读入文件
soil <- read.table('soil.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
group  <- read.table('group.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
soil <- merge(soil, group, by = 'sample')
 
#以 chao1 指数为例，同时将分组列转换为因子变量
chao1 <- soil[ ,c('sample', 'site', 'chao1')]
chao1$site <- factor(chao1$site)
str(chao1)
head(chao1)
#QQ-plot 检查数据是否符合正态分布（所有的点都离直线很近，落在置信区间内说明正态性良好）
library(car)
qqPlot(lm(chao1~site, data = chao1), simulate = TRUE, main = 'QQ Plot', labels = FALSE)
#使用 Bartlett 检验进行方差齐性检验（p 值大于 0.05 说明方差齐整）
bartlett.test(chao1~site, data = chao1)

#满足假设，单因素方差分析，详情使用?aov查看帮助，
fit <- aov(chao1~site, data = chao1)
summary(fit)
 
#若想查看各组均值及标准差，可使用 aggregate()
chao1_mean <- aggregate(chao1$chao1, by = list(chao1$site), FUN = mean)
chao1_sd <- aggregate(chao1$chao1, by = list(chao1$site), FUN = sd)

##方差分析后，多重比较，继续探寻两两分组间的差异
#Tukey HSD 检验 (在ANOVA结果的基础上继续执行)
tuk <- TukeyHSD(fit, conf.level = 0.95)
plot(tuk)

##multcomp包中提供了更直观的方法，展示Tukey检验的结果。
library(multcomp)
tuk <- glht(fit, alternative = 'two.sided', linfct = mcp(site = 'Tukey'))
plot(cld(tuk, level = 0.05, decreasing = TRUE))


### ggplot2柱状图示例
#ggplot2 柱状图示例
dat <- merge(chao1_mean, chao1_sd, by = 'Group.1')
names(dat) <- c('group', 'mean', 'sd')
dat <- cbind(dat, sign = c('a', 'b', 'b'))
 
library(ggplot2)
 
ggplot(dat, aes(group, mean)) +
geom_col(aes(fill = group), width = 0.4, show.legend = FALSE) +
geom_errorbar(aes(ymax = mean + sd, ymin = mean - sd), width = 0.15, size = 0.5) +
geom_text(aes(label = sign, y = mean +sd + 200)) +
theme(panel.grid = element_blank(), panel.background = element_rect(color = 'black', fill = 'transparent'), plot.title = element_text(hjust = 0.5)) +
labs(x = 'Group', y = 'Chao1', title = 'Tukey HSD test')
```

### 单因素协方差（有参）

```R
#当方差分析中存在协变量时，即可称为协方差分析。其中单因素协方差分析是最常见的，在单因素方差分析中引入了协变量。ANCOVA除了要求数据服从正态分布，以及各组方差相等外，还假定回归斜率相同（回归斜率的同质性)

#读入文件
soil <- read.table('soil.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
group  <- read.table('group.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
soil <- merge(soil, group, by = 'sample')
 
#以 shannon 指数为例，同时将分组列转换为因子变量
shannon <- soil[ ,c('sample', 'treat', 'shannon', 'days')]
shannon$treat <- factor(shannon$treat)
str(shannon)
head(shannon)
#QQ-plot 检查数据是否符合正态分布
library(car)
qqPlot(lm(shannon~treat, data = shannon), simulate = TRUE, main = 'QQ Plot', labels = FALSE)
#使用 Bartlett 检验进行方差齐性检验（p 值大于 0.05 说明方差齐整）
bartlett.test(shannon~treat, data = shannon)

#这里ANCOVA包含植物生长时间×化学物质类型的交互项，需对回归斜率的同质性进行检验，若交互效应显著，则意味着植物生长时间和植物根际菌群的Shannon指数的关系依赖于所添加的化学物质类型
#检验回归斜率的同质性，交互效应不显著则支持斜率相等的假设（即 p 值大于 0.05 说明回归斜率相等）
summary(aov(shannon~days*treat, data = shannon))
#这里其实就是双因素方差分析  

#对于带协变量的项，以单因素协方差为例，aov()函数书写为aov(y~x+A)的样式，其中x为协变量，A为因子变量，注意需要将协变量写在因子前面

#查看各组均值及标准差
aggregate(shannon$shannon, by = list(shannon$treat), FUN = mean)
aggregate(shannon$shannon, by = list(shannon$treat), FUN = sd)
#由于使用了协变量，若想获取去除协变量效应后的组均值（调整的组均值）
library(effects)
effect('treat', fit)

#HH 包 ancova() 可绘制因变量、协变量和因子之间的关系图
#详情使用 ?ancova 查看帮助
library(HH)
ancova(shannon~days+treat, data = shannon) （双因素协）
ancova(shannon~days*treat, data = shannon)（双因素方）
#同时通过关系图可知，3条回归线相互平行，只是截距项不同，b组截距项最大，c组截距项最小；回归线拟合效果并不理想
#我们并不是想做个双因素ANOVA分析，而是在更改了函数公式后，意在可视化允许斜率和截距项依据组别而发生改变，从而体现那些违背回归斜率同质性的实例
```

### 单因素协方差（非参）

```R
#ANCOVA除了要求数据服从正态分布，以及各组方差相等外，还假定回归斜率相同（回归斜率的同质性);若不满足，可用非参
#读入文件，添加分组信息
soil <- read.table('soil.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
group  <- read.table('group.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
soil <- merge(soil, group, by = 'sample')
 
#以 shannon 指数为例，同时将分组列转换为因子变量
shannon <- soil[ ,c('sample', 'treat', 'shannon', 'days')]
shannon$treat <- factor(shannon$treat)
str(shannon)
head(shannon)

#sm 包 sm.ancova()，详情使用 ?sm.ancova 查看详情
library(sm)
sm.ancova(x = shannon$days, y = shannon$shannon, group = shannon$treat, model = 'equal')
# x为协变量，y为因变量，group为分组因子，method定义参考模型（详见帮助）。
```

### 双因素（有参）

```R
#双因素方差分析，顾名思义，两组因子变量对应一组因变量。;先进行正态性检验和方差齐性检验，如果不满足上述前提假设，一是可以考虑转化数据（当然，我们需要确保转换后的数据能够被合理解释，否则将无意义）；二是可以考虑使用非参数的检验方法，对于双因素方差分析的非参数替代方法，常使用Scheirer-Ray-Hare检验

#读入文件，添加分组信息
soil <- read.table('soil.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
group  <- read.table('group.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
soil <- merge(soil, group, by = 'sample')
 
#以 chao1 指数为例，同时将分组列转换为因子变量
chao1 <- soil[ ,c('sample', 'treat', 'times', 'chao1')]
chao1$treat <- factor(chao1$treat)
chao1$times <- factor(chao1$times)
str(chao1)
head(chao1)

#QQ-plot 检查数据是否符合正态分布,这里需要检查两组（化学物质类型、处理时间）是否均满足。
par(mfrow = c(1, 2))
qqPlot(lm(chao1~treat, data = chao1), simulate = TRUE, main = 'QQ Plot', labels = FALSE)
qqPlot(lm(chao1~times, data = chao1), simulate = TRUE, main = 'QQ Plot', labels = FALSE)

#使用 Bartlett 检验进行方差齐性检验（p 值大于 0.05 说明方差齐整）
bartlett.test(chao1~treat, data = chao1)
bartlett.test(chao1~times, data = chao1)

#满足假设，双因素方差分析，详情使用 ?aov 查看帮助
fit <- aov(chao1~treat*times, data = chao1)
summary(fit)
 
#上式等同于
fit <- aov(chao1~treat+times+treat:times, data = chao1)
summary(fit)
 
#查看各组均值及标准差
aggregate(chao1$chao1, by = list(chao1$treat, chao1$times), FUN = mean)
aggregate(chao1$chao1, by = list(chao1$treat, chao1$times), FUN = sd)
###双因素ANOVA的aov()函数书写为aov(y~A*B)的样式，表示考虑所有可能的交互项：A、B以及A和B的交互（A:B），其中A、B分别为两组因子变量。因此，双因素ANOVA的aov()函数也可书写为aov(y~A+B+A:B)的样式。此外，表达式中各效应存在主次顺序，即y~A*B与y~B*A的处理方式是不同的，一般情况下，越基础性的变量越应放在表达式前面。在这里，主因素（treat）在前，次因素（times）在后

#展示双因素ANOVA的交互效应，以查看数据分布
#例如，interaction.plot() 函数，展示各组均值趋势
interaction.plot(chao1$times, chao1$treat, chao1$chao1)

#再例如，boxplot() 函数，以箱线图展示各组数据分布
boxplot(chao1~treat*times, data = chao1, col = c('#B3DE69', '#FDB462', '#80B1D3'))

#再例如，gplots 包 plotmeans() 函数，展示了均值和误差棒（95% 置信区间，以及各组样本量大小
library(gplots)
plotmeans(chao1~interaction(treat, times), data = chao1, connect = list(c(1, 4, 7, 10), c(2, 5, 8, 11), c(3, 6, 9, 12)))

#再例如，HH 包 interaction2wt() 函数
library(HH)
interaction2wt(chao1~treat*times, data = chao1)
```

### 双因素（非参）

```R
#双因素ANOVA建立在两组数据必须均服从正态分布，以及各组方差相等的基础上才能执行，如前文双因素ANOVA中的方法所述。若二者之一不符合，Scheirer-Ray-Hare检验将会是一种实用的方法。

#读入文件，添加分组信息
soil <- read.table('soil.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
group  <- read.table('group.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
soil <- merge(soil, group, by = 'sample')
 
#以 chao1 指数为例，同时将分组列转换为因子变量
chao1 <- soil[ ,c('sample', 'treat', 'times', 'chao1')]
chao1$treat <- factor(chao1$treat)
chao1$times <- factor(chao1$times)
str(chao1)
head(chao1)

#rcompanion 包 scheirerRayHare()，详情使用 ?scheirerRayHare 查看帮助说明
library(rcompanion)
scheirerRayHare(chao1~treat*times, data = chao1)
# 式中表达方式类似于方差分析的aov()函数，写作scheirerRayHare(y~A*B)的样式，表示考虑所有可能的交互项：A、B以及A和B的交互（A:B），其中A、B分别为两组因子变量。表达式中各效应存在主次顺序，主因素（treat）在前，次因素（times）在后。
# 当数据满足ANOVA的条件时，尽可能使用ANOVA分析，能够有效地鉴别出非参数检验鉴别不到的差异；当无法适用ANOVA时，再考虑非参数的方法。对于双因素中交互项的可视化，同样可参考前文双因素ANOVA中的方法
```

### 重复测量ANOVA分析

```R
#所谓重复测量方差分析（重复测量ANOVA），即受试者被测量不止一次。根据分组因子数量的不同，可再分为单因素、双因素、多因素重复测量方差分析等。
#使用R自带的CO2数据集
data(CO2)
w1b1 <- subset(CO2, Treatment == 'chilled') #我们只关注其中的寒带植物
 
#将分组转变为因子类型
w1b1$Type <- factor(w1b1$Type)
w1b1$conc <- factor(w1b1$conc)

#重复测量ANOVA同样要求数据服从正态分布，以及各组方差相等
#重复测量方差分析
#含单个组内因子（W）和单个组间因子（B）的重复测量方差分析的aov()函数书写为aov(y~B*W+Error(Subject/W))的样式
fit <- aov(uptake ~ conc*Type + Error(Plant/conc), data = w1b1)
summary(fit)

#交互效应展示示例
boxplot(uptake~Type*conc, data = w1b1, col = c('red', 'blue'), las = 2)
```

### 多元方差分析（有参）

[R进行三因素方差分析 (qq.com)](https://mp.weixin.qq.com/s?__biz=Mzg3MzQzNTYzMw==&mid=2247494090&idx=1&sn=2940b1353311a738101a0eb659606579&chksm=cee2b154f9953842fa833d09b33bdca57afad19952c5c1f8f3e3e5e6ec34c537b36c7b27b9fa&scene=178&cur_album_id=1650388742987153409#rd)

![image-20220424171352084](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202204241713282.png)

```R
#因子变量存在多组时，即为多因素方差分析（因子变量越多，解释起来也就越复杂，所以一般不会涉及更多因素）；存在协变量时，称为协方差分析。
#读入文件，添加分组信息
soil <- read.table('soil.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
group  <- read.table('group.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
soil <- merge(soil, group, by = 'sample')
 
#选择数据，并将分组列转换为因子变量
muti <- soil[ ,c('sample', 'treat', 'chao1', 'pH', 'NR')]
muti$treat <- factor(muti$treat)
str(muti)
head(muti)

#单因素MANOVA有两个前提假设，一是多元正态性，二是方差-协方差矩阵同质性
#QQ-plot 检验多元正态性
y <- cbind(muti$NR, muti$pH, muti$chao1)
coord <- qqplot(qchisq(ppoints(nrow(y)), df = ncol(y)), mahalanobis(y, colMeans(y), cov(y)))
abline(a = 0, b = 1)
#可以交互式展示样本位置，可用于观测离群点
identify(coord$x, coord$y, labels = muti$sample)
#交互式地在图中点击这两个点，查看它们是那些样本;若有必要，可以将这两个样本剔除，然后再继续下一步

#方差-协方差矩阵同质性即指各组的协方差矩阵相同，通常可使用Box’s M检验来评估该假设。注：Box’s M检验对正态性假设很敏感。
#Box's M 检验验证方差-协方差矩阵同质性（p 值大于 0.05 即说明各组的协方差矩阵相同）
library(biotools)
boxM(muti[ ,c('chao1', 'pH', 'NR')], muti[ ,'treat'])
#假设通过，执行单因素 MANOVA，详情使用 ?manova 查看帮助。
fit <- manova(cbind(NR, pH, chao1)~treat, data = muti)
summary(fit) #查看整体结果
summary.aov(fit) #对每一个变量做单因素方差分析

#假设未通过，可使用稳健单因素 MANOVA
#通过 rrcov 包中的 Wilks.test() 函数实现，详情可使用 ?Wilks.test 查看帮助
library(rrcov)
muti$treat <- factor(muti$treat)
Wilks.test(treat~., data = muti[c('treat', 'NR', 'pH', 'chao1')], method = 'c')
```

### 多元方差分析（无参）

```R
#当因变量不止一个时，即一个或多个因子变量对应了多个因变量时，可使用多元方差分析（MANOVA）。但是MANOVA的条件非常苛刻，以前文简述的单因素MANOVA为例，要求数据满足多元正态性、方差-协方差矩阵同质性，大部分案例中都是直接拒绝的。
#替代方法可以使用稳健MANOVA，这个在前文单因素MANOVA中已经提到。对于非参数的方法，常用置换多元方差分析（PERMANOVA）。
#读入文件，添加分组信息

soil <- read.table('soil.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
group  <- read.table('group.txt', sep = '\t', header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
soil <- merge(soil, group, by = 'sample')
 
#选择数据，并将分组列转换为因子变量
muti <- soil[ ,c('sample', 'treat', 'chao1', 'pH', 'NR')]
muti$treat <- factor(muti$treat)

#置换多元方差分析（PERMANOVA），vegan 包 adonis()，详情使用 ?adonis 查看帮助
library(vegan)
 
rownames(muti) <- muti$sample
muti_dat <- muti[ ,c('chao1', 'pH', 'NR')]
#muti_dat <- decostand(muti_dat, 'standardize')   #可选 z-score 标准化，消除量纲差异
dis <- vegdist(muti_dat, method = 'euclidean')      #获取欧式距离矩阵（这里以欧式距离为例，实际分析中视情况选择合适的距离测度）
adonis(dis~treat, muti, permutations = 999)  #999 次置换获得 p 值（其实就是通过置换检验来实现）

'''
以上示例中只有一组分组因子。当存在多组分组时（多因素时），adonis()仍然适用。
'''
#类似于 aov()，多因素下，在 ~ 后面添加分组因子即可
#例如 A 为协变量，B 为自变量时，注意协变量在前，自变量在后
adonis(y~A+B, data, permutations = 999)
 
#例如考虑 A、B 双因素及交互作用时，注意主因素在前，次因素在后；多因素时以此类推
adonis(y~A*B, data, permutations = 999)
```

### AMOVA分析

```R
'''
AMOVA（Analysis of molecular variance）是一种非参数分析方法，基于对象间距离检验组间差异的显著性。
AMOVA最初是针对DNA单倍型实施的，利用分子标记检测种群分化，通过进化距离（evolutionary distance）度量并计算单倍型（含等位基因）或基因型间的方差（δ2），推断群体遗传变异水平。后经拓展，可适用于任何标记系统（marker system），例如AFLP、SNP或SSR等标记，广泛用于遗传多样性和群体遗传结构研究中。
'''

library(ade4)
 
data(humDNAm)
 
names(humDNAm)
as.matrix(humDNAm$distance)[1:6,1:6]
head(humDNAm$samples)
head(humDNAm$structures)

#初步计算，详情 ?amova
#注：amova() 中输入的距离矩阵必须为欧氏距离属性，否则会报错
amovahum <- amova(samples = humDNAm$samples, distances = sqrt(humDNAm$distances), structures = humDNAm$structures)
amovahum

#例如
amovahum$result
amovahum$componentsofcovariance
amovahum$statphi
 
write.table(amovahum$componentsofcovariance, 'componentsofcovariance.txt', sep = '\t', col.names = NA, quote = FALSE)

#这里随机置换 99 次为例，详情 ?randtest
randtesthum <- randtest(amovahum, nrepet = 99)
randtesthum
 
plot(randtesthum)

#例如
summary(randtesthum)
 
randtesthum$expvar
randtesthum$adj.pvalue
randtesthum$plot

###amova分析输入的距离矩阵必须是欧式距离属性
若是其他的种类的距离矩阵，做个转化
distance是某种非欧式属性的距离
平方根转换
is.euclid(distance^0.5)
```

## 置换检验

置换检验（Permutation Test），也称随机化检验或重随机化检验，提供了稳健的检验框架

某些差异分析方法建立在观测数据服从较好的特定理论分布的基础上，如常见的参数检验T检验、方差分析等，假定观测数据抽样自正态分布；再如分析差异表达基因时最常用的负二项分布模型，假定整体基因表达具有负二项分布特征。在这些情况下，根据数据的理论分布，即可比较容易建立假设检验和总体参数的置信区间估计。
但在许多情况中，统计检验并不一定满足，比如数据抽样于未知或混合分布、样本量过小、存在离群点、基于理论分布设计合适的统计检验过于复杂且数学上难以处理等情况，这时基于随机化的统计方法就可以派上用场。

尽管传统的T检验和置换检验过程都计算了相同的t统计量，但置换方法并不是将统计量与理论分布进行比较，而是将其与置换观测数据后获得的经验分布进行比较，根据统计量的极端性判断是否有足够的理由拒绝零假设。如果数据偏离正态分布，存在离群值，或者感觉对于标准的参数方法来说数据集太小，那么置换检验则是一个不错的选择。

此外，对于执行置换检验的软件，如R，也可以通过设置随机数种子，实现结果重现的目的
依靠基础的抽样分布理论知识，置换检验提供了另外一种强大的可选检验思路，由于其不受数据分布特征的限制，因此它的应用更为灵活。置换检验主要用于生成检验零假设的p值，有助于回答“效应是否存在”，但对于获取置信区间和估计测量精度则比较困难。

```
#数据获取自 http://blog.sciencenet.cn/blog-3406804-1173120.html
dat <- read.table('data.txt', sep = '\t', row.names = 1, header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
 
#只包含细菌类群丰度数据，计算细菌类群之间的相关性
dat_phylum <- dat[8:17]
var_name <- names(dat_phylum)
 
##自写过程实现
#计算观测数据的相关系数
dat_phylum <- scale(dat_phylum)
cor0 <- cor(dat_phylum, method = 'pearson')
 
p_num <- cor0
p_num[abs(p_num)>0] <- 1
 
#随机置换数据 999 次，计算每次获得的相关系数，并统计 |corN|>|cor0| 的频数
set.seed(123)
 
for (i in 1:999) {
        random <- matrix(sample(dat_phylum), ncol = 10)
        colnames(random) <- var_name
        corN <- cor(random, method = 'pearson')
        
        corN[abs(corN) >= abs(cor0)] <- 1
        corN[abs(corN) < abs(cor0)] <- 0
        p_num <- p_num + corN
}
 
#p 值矩阵，即 |corN|>|cor0| 的概率
p <- p_num/1000
p
 
##使用 R 内置函数的相关性系数计算并获得检验
library(psych)
 
cor_test <- corr.test(dat_phylum, method = 'pearson')
 
cor_test$r
cor_test$p
 
##相关图比较
#左图为手写的置换检验结果，右图为 psych 包获得的结果
#仅显著的相关系数标以背景色
library(corrplot)
 
layout(matrix(c(1,2), 1, 2, byrow = TRUE))
corrplot(cor0, method = 'square', type = 'lower', p.mat = p, sig.level = 0.05, insig = 'blank', addCoef.col = 'black', diag = FALSE, number.cex = 0.8, tl.cex = 0.8)
corrplot(cor_test$r, method = 'square', type = 'lower', p.mat = cor_test$p, sig.level = 0.05, insig = 'blank', addCoef.col = 'black', diag = FALSE, number.cex = 0.8, tl.cex = 0.8)
```

### 自助法

[自助法(bootstrap)统计检验中的应用](https://mp.weixin.qq.com/s/NAs4g0h4VTOs0FKyYW1CGg)

```
'''
Bootstrap（自助法、自举法）是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法。
Bootstrap通过对给定数据集进行有放回的重抽样以创建多个模拟数据集，生成一系列待检验统计量的经验分布，可以计算标准误差、构建置信区间并对多种类型的样本统计信息进行假设检验。
Bootstrap无需假设一个特定的理论分布，便可生成统计量的置信区间并能检验统计假设，更易于理解以及适用于更多条件，因此常作为传统假设检验的替代方法。
'''
#但若样本的潜在分布未知，或存在离群点，或样本量过小，以及没有其它合适的参数方法时，bootstrap将是获取置信区间以及进行假设检验的一种有效方法。例如期望估计样本中位数的置信区间，或者两样本的中位数之差，正态理论没有现成的简单公式可直接套用，此时bootstrap将会是不错的选择。

#数据获取自 http://blog.sciencenet.cn/blog-3406804-1173120.html
dat <- read.table('data.txt', sep = '\t', row.names = 1, header = TRUE, stringsAsFactors = FALSE, check.names = FALSE)
 
#只包含细菌类群丰度数据，计算细菌类群之间的相关性，以 pearson 相关系数为例
dat_phylum <- dat[8:17]
dat_phylum <- scale(dat_phylum)
 
cor0 <- cor(dat_phylum, method = 'pearson')
cor0    #相关矩阵
 
#以下使用 bootstrap 的方法获取 pearson 相关系数的置信区间，用以评估观测值的相关系数是否是可信的
 
#对于 boot() 而言，如果只有单个统计量，函数应当返回一个数值；如果存在多个统计量，函数应该返回一个向量
#这里需要计算多变量间的相关系数，即代表了多统计量情形，因此首先创建一个能够返回相关系数向量的函数
library(reshape2)
 
cor_prarson <- function(data, indices) {
    cor0 <- melt(cor(data[indices, ]))
    cor0 <- cor0$value
    cor0
}
 
#自助抽样 1000 次，详情 ?boot
library(boot)
set.seed(123)
 
boot_result <- boot(data = dat_phylum, statistic = cor_prarson, R = 1000)
boot_result

#根据一开始计算的相关矩阵，获取索引
cor0 <- melt(cor0)
group <- paste(cor0$Var1, cor0$Var2, sep = '-')
 
#查看感兴起微生物类群间相关系数的抽样分布
plot(boot_result, index = which(group == 'Acidobacteria-Proteobacteria'))

#获取 95% 置信区间，详情 ?boot.ci
#type 参数设定获取置信区间的方法，这里展示了所有方法的区间估计结果，细节描述见帮助
boot.ci(boot_result, conf = 0.95, type = 'all', index = which(group == 'Acidobacteria-Proteobacteria'))

#获取 95% 置信区间，详情 ?boot.ci
#type 参数设定获取置信区间的方法，这里展示了所有方法的区间估计结果，细节描述见帮助
boot.ci(boot_result, conf = 0.95, type = 'all', index = which(group == 'Acidobacteria-Proteobacteria'))

#对于原始数据集所观测的两类群间的 pearson 相关系数
cor0[which(cor0$Var1 == 'Acidobacteria' & cor0$Var2 == 'Proteobacteria'), ]

#上述结果表明 Acidobacteria 和 Proteobacteria 的相关性显著
#再看一个不显著的例子，Actinobacteria 和 Proteobacteria
boot.ci(boot_result, conf = 0.95, type = 'all', index = which(group == 'Actinobacteria-Proteobacteria'))
cor0[which(cor0$Var1 == 'Actinobacteria' & cor0$Var2 == 'Proteobacteria'), ]
```

## 聚类

[R语言做聚类分析Kmeans时确定类的个数](http://blog.sina.cn/dpool/blog/s/blog_13ea9a2450102xkcz.html)

[R语言中聚类确定最佳K值之Calinsky criterion](https://www.cnblogs.com/xiaomingzaixian/p/9270301.html)

[【机器学习】确定最佳聚类数目的10种方法](https://zhuanlan.zhihu.com/p/24546995)

[maSigPro包:时间序列数据处理工具](https://www.jianshu.com/p/b4ed31cfcd91)

[层次分划—双向指示种分析（TWINSPAN）及其在R中的计算](https://mp.weixin.qq.com/s?__biz=MzIxNzc1Mzk3NQ==&mid=2247484601&idx=1&sn=0c95c8ceb0b4375944d53e96220e9308&chksm=97f5b4a1a0823db749b116a58f6d5cb32adcff7afa8a92cf5151f3d87932e5649cb2f598ffa2&token=66128012&lang=zh_CN&scene=21#wechat_redirect)

[层次聚类结果的比较和评估及R操作](https://mp.weixin.qq.com/s?__biz=MzIxNzc1Mzk3NQ==&mid=2247484601&idx=2&sn=906d2450119e06ec1521392779d883a3&chksm=97f5b4a1a0823db75e0eb5799ab316dfbc7a4b5fa7df6f36d97ec673b0f9dbfeee1634bce91a&token=66128012&lang=zh_CN&scene=21#wechat_redirect)

 [R语言聚类分析--cluster, factoextra ](https://mp.weixin.qq.com/s/9EChqbVmFrNWi_Siu4hdqQ)

```
## R语言聚类分析Kmeans时候确定类的个数

n = 100
g = 6
set.seed(g)
d <- data.frame(x = unlist(lapply(1:g, function(i) rnorm(n/g, runif(1)*i^2))), y = unlist(lapply(1:g, function(i) rnorm(n/g, runif(1)*i^2))))
plot(d)

mydata <- d
#这里的wss(within-cluster sum of squares)是组内平方和
wss <- (nrow(mydata)-1)*sum(apply(mydata, 2, var))

for (i in 2:15){
  wss[i] <- sum(kmeans(mydata,centers = i)$withiness)
}
wss

plot(1:15, wss, type='b', xlab='Number of Clusters',ylab='Within groups sum of squares')

## 方法二:
#使用fpc包里的pamk函数来估计类的个数：
library(cluster)
library(fpc)
pamk.best <- pamk(d)
cat('number of clusters estimated by optimum average silhouette width:', pamk.best$nc, '')
plot(pam(d, pamk.best$nc))


require(vegan)
fit <- cascadeKM(scale(d, center = TRUE, scale = TRUE), 1, 10, iter = 1000)
plot(fit, sortg = TRUE, grpmts.plot = TRUE)
calinski.best <- as.numeric(which.max(fit$results[2,]))
cat('Calinski criterion optimal number of clusters:', calinski.best, '')
...
其他方法看链接内容
```

![image-20201222101201464](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20201222101201464.png)

### 层次聚类

[层次聚合分类分析及其在R中的计算](https://mp.weixin.qq.com/s?__biz=MzIxNzc1Mzk3NQ==&mid=2247484593&idx=1&sn=d8160e2918b0aa4dd90cb10c2f82d266&chksm=97f5b4a9a0823dbf52ebacc1c1390c2047dfafcf217ae9afe34583dfe4a999621d6d274c4af2&token=66128012&lang=zh_CN&scene=21#wechat_redirect)

层次聚类（hierarchical clustering）试图在不同层次对数据集进行划分，从而形成树形的聚类结构。数据集划分可采用“自底向上”的聚合策略，也可采用“自顶向下”的分拆策略。本篇首先简介“自底向上”的聚合策略。

在该方法中影响聚类结果的主要因素包括数据预处理方式、距离测度和聚类算法的选择。

分为

单连接聚合聚类 single linkage agglomerative clustering

完全连接聚合聚类 Complete linkage agglomerative clustering

 平均聚合聚类 average agglomerative clustering

![image-20210125173429765](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20210125173429765.png)

**Ward最小方法聚类Ward’s minimum variance clustering**

是一种基于最小二乘法线性模型准则的聚类方法，分组的依据是使组内误差平方和（每个对象到其组质心的距离的平方和）最小化

**灵活聚类（flexible clustering)**

一种能够涵盖上述聚类方法的模型，该方法中没有固定的聚合策略，通过指定不同的参数项（αh、αi、β、γ）实现不同的聚类效果。

![image-20210125173719724](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20210125173719724.png)

**R中实现**

计算距离

```R
##示例 1，基因表达数据
dat_gene <- read.delim('gene_express.txt', row.names = 1)
dat_gene <- t(dat_gene)
 
#常用欧几里得距离表示各样本中的基因表达相异性，可由 vegan 包 vegdist() 计算距离测度
dis_euc <- vegan::vegdist(dat_gene, method = 'euclidean')
 
##示例 2，物种丰度数据
dat_otu <- read.delim('otu_table.txt', row.names = 1)
dat_otu <- t(dat_otu)
 
#群落数据的分析中，Bray-curtis 距离是常用的距离测度类型
dis_bray <- vegan::vegdist(dat_otu, method = 'bray')
 
#不过考虑到某些聚类方法只能以欧式距离作为输入
#因此可以通过平方根转化的形式，将 Bray-curtis 距离转化为具有欧式几何属性的距离测度
ade4::is.euclid(dis_bray)
dis_bray_euc <- dis_bray^0.5
ade4::is.euclid(dis_bray_euc)
 
##此外，如果提供了现有的已经计算好的某种距离矩阵，也可直接读取
#例如，外部的 Bray-curtis 距离矩阵
dis_bray <- as.dist(read.delim('bray_distance.txt', row.names = 1))
 
#平方根转化，使其具有欧式几何属性
dis_bray_euc <- dis_bray^0.5
```

2.运行层次聚合分类

```
##基于连接的层次聚合分类，详情 ?hclust
#以物种丰度的 Bray-curtis 距离为例
 
#单连接聚合聚类，单联动
clust_single <- hclust(dis_bray, method = 'single')
summary(clust_single)
 
#完全连接聚合聚类，全联动
clust_complete <- hclust(dis_bray, method = 'complete')
summary(clust_complete)
 
#聚类树，默认效果
par(mfrow = c(1, 2))
plot(clust_single, main = 'single')
plot(clust_complete, main = 'complete')
```

平均聚合聚类

```
##平均聚合聚类，详情 ?hclust
#以物种丰度的 Bray-curtis 距离为例
 
#使用中值的非权重成对组法，UPGMA
clust_average <- hclust(dis_bray, method = 'average')
summary(clust_average)
 
#使用质心的非权重成对组法，UPGMC
clust_centroid <- hclust(dis_bray, method = 'centroid')
summary(clust_centroid)
 
#使用中值的权重成对组法，WPGMA
clust_mcquitty <- hclust(dis_bray, method = 'mcquitty')
summary(clust_mcquitty)
 
#使用质心的权重成对组法，WPGMC
clust_median <- hclust(dis_bray, method = 'median')
summary(clust_median)
 
#聚类树，默认效果
par(mfrow = c(2, 2))
plot(clust_average, main = 'UPGMA')
plot(clust_centroid, main = 'UPGMC')
plot(clust_mcquitty, main = 'WPGMA')
plot(clust_median, main = 'WPGMC')
```

![image-20210125174324889](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20210125174324889.png)

Ward最小方差聚类

```R
##Ward 最小方差聚类（包含两种不同的 Ward 聚类算法，详情 ?hclust）
#该方法中只能使用欧式几何属性的距离测度，因此更换为平方根转化后的 Bray-curtis 距离测度
 
#ward.D
clust_ward <- hclust(dis_bray_euc, method = 'ward.D')
summary(clust_ward)
 
#ward.D2
clust_ward2 <- hclust(dis_bray_euc, method = 'ward.D2')
summary(clust_ward2)
 
#聚类树，默认效果
par(mfrow = c(1, 2))
plot(clust_ward, main = 'ward.D')
plot(clust_ward2, main = 'ward.D2')
```

灵活聚类

```
##灵活聚类，详情 ?agnes
#也可在该函数中通过指定 method 执行上述的多种常规聚类，如 method='average' 即为 UPGMA
#当 method='flexible' 时，par.method 参数中的 4 个值分别对应了 αh、αi、γ、β 的设定值，例如
clust_flex <- cluster::agnes(dis_bray_euc, method = 'flexible', par.method = c(0.5, 0.5, 0, 0.5))
summary(clust_flex)
 
plot(clust_flex)
```



### 划分聚类

**K均值划分(k-means)**

[划分聚类—k均值划分和围绕中心点划分及R操作](https://mp.weixin.qq.com/s?__biz=MzIxNzc1Mzk3NQ==&mid=2247484607&idx=1&sn=c69c7161f3b6e31433dc52afc68a7831&chksm=97f5b4a7a0823db1adfd991e10127ab965777d153d9aaeb9ce5a0d99d872f47f2e6bfd7d805e&token=66128012&lang=zh_CN&scene=21#wechat_redirect)

```R
#随机生成模拟数据
set.seed(123)
rand <- rbind(matrix(rnorm(150, mean = 10, sd = 0.3), ncol = 5),
              matrix(rnorm(150, mean = 3, sd = 0.2), ncol = 5),
              matrix(rnorm(150, mean = 1, sd = 0.1), ncol = 5),
              matrix(rnorm(150, mean = 6, sd = 0.3), ncol = 5),
              matrix(rnorm(150, mean = 9, sd = 0.3), ncol = 5))

#NbClust 包的最佳聚类数量评估，详情 ?NbClust
library(NbClust)
nc <- NbClust(rand, distance = 'euclidean', min.nc = 2, max.nc = 10, method = 'kmeans', index = 'hubert')
plot(nc)
#vegan 包的最佳聚类数量评估，详情 ?cascadeKM
library(vegan)
cc <- cascadeKM(rand, inf.gr = 2, sup.gr = 10, iter = 100, criterion = 'ssi')
plot(cc)

```



**围绕中心店划分(PAM)**

[避免聚类中错误识别的不存在的类](https://mp.weixin.qq.com/s?__biz=MzIxNzc1Mzk3NQ==&mid=2247484616&idx=3&sn=15cadafffc71d3607919dc2be9393f6c&chksm=97f5b4d0a0823dc6ede639ed47dab212f7f00deaa5c303ff404b05d96863b79b0b8efbb8267c&token=356373910&lang=zh_CN&scene=21#wechat_redirect)

```
#模拟生成均值为 3、标准差为 0.2 的正态分布的随机数
#共计 200 个随机数，构成 100 行 2 列的矩阵，视为 100 个观测对象，2 个变量
set.seed(123)
rand <- matrix(rnorm(200, mean = 3, sd = 0.2), ncol = 2)
head(rand)

NbClust 包的最佳聚类数量评估，详情 ?NbClust
#此外，层次聚类也可使用该方法评估合适的聚类簇数量
library(NbClust)
 
nc <- NbClust(rand, distance = 'euclidean', min.nc = 2, max.nc = 10, method = 'kmeans', index = 'hubert')
plot(nc)
```

![image-20210125180103102](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20210125180103102.png)

评估显示，大约在k=6或7处有一个明显的拐点。就以产生6个聚类簇为例，执行k-means聚类

```
#k-means 聚类
set.seed(123)
rand_kmeans <- kmeans(rand, centers = 6, nstart = 25)
 
rand_kmeans$cluster
 
#轮廓图评估
library(cluster)
 
plot(silhouette(rand_kmeans$cluster, vegdist(rand, method = 'euclidean')))
```



![image-20210125180124602](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20210125180124602.png)

```R
#二维变量的散点图，并标记分类
library(ggplot2)
library(plyr)
 
rand1 <- data.frame(rand)
rand1$group <- as.character(rand_kmeans$cluster)
group_border <- ddply(rand1, 'group', function(df) df[chull(df[[1]], df[[2]]), ])
 
ggplot(rand1, aes(X1, X2)) +
geom_point(aes(color = group)) +
theme(panel.grid = element_blank(), panel.background = element_rect(color = 'black', fill = 'transparent'), legend.key = element_rect(fill = 'transparent')) +
geom_polygon(data = group_border, aes(fill = group), alpha = 0.1, show.legend = FALSE)
```

![image-20210125180146278](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20210125180146278.png)

《R语言实战》第二版359页提到，NbClust包中的立方聚类规则（Cubic Cluster Criteria，CCC）可以帮助我们揭示不存在的结构。

```
#识别错误的类
library(NbClust)
 
nc <- NbClust(rand, distance = 'euclidean', min.nc = 2, max.nc = 10, method = 'kmeans', index = 'ccc')
plot(nc$All.index, type = 'o', col = 'blue', xlab = 'Number of clusters', ylab = 'CCC')
```

### 模糊聚类

[模糊c均值聚类（FCM）及其在R中实现](https://mp.weixin.qq.com/s?__biz=MzIxNzc1Mzk3NQ==&mid=2247484616&idx=2&sn=8e650c5ab700203fa296d2716c060ac9&chksm=97f5b4d0a0823dc69abb9f2f1d5b2dd7cdbf09316fcc3157f6436ff705ff63e89e8261c4613a&token=66128012&lang=zh_CN&scene=21#wechat_redirect)

先前所简介的[层次聚类](https://mp.weixin.qq.com/s?__biz=MzIxNzc1Mzk3NQ==&mid=2247484593&idx=1&sn=d8160e2918b0aa4dd90cb10c2f82d266&chksm=97f5b4a9a0823dbf52ebacc1c1390c2047dfafcf217ae9afe34583dfe4a999621d6d274c4af2&token=419370574&lang=zh_CN&scene=21#wechat_redirect)、[划分聚类](https://mp.weixin.qq.com/s?__biz=MzIxNzc1Mzk3NQ==&mid=2247484607&idx=1&sn=c69c7161f3b6e31433dc52afc68a7831&chksm=97f5b4a7a0823db1adfd991e10127ab965777d153d9aaeb9ce5a0d99d872f47f2e6bfd7d805e&token=419370574&lang=zh_CN&scene=21#wechat_redirect)等，所产生的聚类簇都是非重叠的实体，即对于每个对象而言只有一个确定的分类，这种聚类方法也成硬聚类（hard-clustering）。

模糊聚类（fuzzy clustering，或称软聚类，soft-clustering）是涉及对象之间的模糊界限时按一定要求对对象分类的数学方法，用于定量地确定对象间的亲疏关系，达到客观准确的分类目的



## 多变量排序分析



[排序分析简介](https://mp.weixin.qq.com/s?__biz=MzAwOTg5ODYyNQ==&mid=2247484064&idx=1&sn=1ef21a14e077128c6b9d373a905ee99e&chksm=9b59dd4eac2e5458871af146e9b5fe7c85947c0cf3ebb75880b18a897abf3dedcd1ab38141a3&mpshare=1&scene=1&srcid=&sharer_sharetime=1591547318282&sharer_shareid=59b64c339c4e339bb243a53b299dcf8d&key=983943efdc6388ddfc3c7ab4677aa5f8aca09b18bbe6c3eb13769eb1d5dd9d7fe3df7379cbb2d48564056b6e0cbb71bff8ff9b41309e35e5608bdcb97519dbb6f57a3781cd64c2fc4d4b0e064064ee7c&ascene=1&uin=Mjc2NjUwNjgzOA%3D%3D&devicetype=Windows+10+x64&version=62090070&lang=zh_CN&exportkey=Awbo2jQq6WoFdR7ln9U%2Bbu8%3D&pass_ticket=OgFI03cbOodZF9BCd6N4rzrUDZTAvGjmnuNy2Ome2pNqRzYbuy8inleYEzRU7qCf)

[PCA、PCoA和NMDS有什么区别](https://www.omicshare.com/forum/thread-6351-1-1.html)

- **在非限制性排序中，16S和宏基因组数据分析通常用到的是PCA分析和PCoA分析。两者的区别在于：****PCA分析是基于原始的物种组成矩阵所做的排序分析，而PCoA分析则是基于由物种组成计算得到的距离矩阵得出的。
- 如果大于4.0，就应该选单峰模型；
- 如果3.0-4.0之间，选线性模型或者单峰模型均可；
- 如果小于3.0, 线性模型的结果要好于单峰模型
- 查看结果中的“Axis lengths”的第一轴DCA1的值，根据该值判断该采用线性模型还是单峰模型：

```R
#判断选择用单峰模型还是线性模型 decorana()
install.packages('vegan')
library('vegan')
data(varespec)
vare_dca<-decorana(varespec)
vare_dca
```

**输入文件一般都为分组文件及丰度表**

### **PLS-DA分析**

```R
library(mixOmics)
library(ggplot2)

##读入文件
#门水平丰度表
phylum <- read.delim('phylum_table.txt', row.names = 1, sep = '\t', stringsAsFactors = FALSE, check.names = FALSE)
phylum <- data.frame(t(phylum))

#样本分组文件
group <- read.delim('group.txt', sep = '\t', stringsAsFactors = FALSE)

##PLS-DA 分析
#基于门水平丰度表，只展示前 3 个排序轴
phylum <- phylum[group$names, ]
plsda_result <-plsda(phylum, group$group, ncomp = 3)

#简要查看结果
plsda_result
#或
names(plsda_result)

#查看排序轴解释量
plsda_result$explained_variance$X
#查看样本排序坐标
plsda_result$variates$X
#查看细菌门类群排序坐标
plsda_result$loadings$X

#使用 plotIndiv() 绘制 PLS-DA 分析结果
plotIndiv(plsda_result, ind.names = TRUE, style = 'ggplot2')
plotIndiv(plsda_result, ind.names = TRUE, style = '3d')

#提取坐标轴解释量（前两轴）
plsda_result_eig <- {plsda_result$explained_variance$X}[1:2]

#提取样本点坐标（前两轴）
sample_site <- data.frame(plsda_result$variates)[1:2]

#为样本点坐标添加分组信息
sample_site$names <- rownames(sample_site)
names(sample_site)[1:2] <- c('plsda1', 'plsda2')
sample_site <- merge(sample_site, group, by = 'names', all.x = TRUE)

#可选输出各样本的 PLS-DA 分析结果
write.table(sample_site, 'plsda_sample.txt', row.names = FALSE, sep = '\t', quote = FALSE)

#使用 ggplot2 简单绘制 PLS-DA 结果图
plsda_plot <- ggplot(sample_site, aes(plsda1, plsda2, color = group, label = names)) +
geom_point(size = 1.5, alpha = 0.6) + 
stat_ellipse(show.legend = FALSE) +	#添加 95% 置信椭圆
scale_color_manual(values = c('#1D7ACC', '#F67433', '#00815F')) +
theme(panel.grid = element_line(color = 'grey50'), panel.background = element_rect(color = 'black', fill = 'transparent')) + 
theme(legend.title = element_blank(), legend.key = element_rect(fill = 'transparent')) +
labs(x = paste('PLS-DA axis1 ( explained variance ', round(100 * plsda_result_eig[1], 2), '% )', sep = ''), y = paste('PLS-DA axis2 ( explained variance ', round(100 * plsda_result_eig[2], 2), '% )', sep = ''))

#ggsave('plsda_plot.pdf', plsda_plot, width = 6, height = 5)
ggsave('plsda_plot.png', plsda_plot, width = 6, height = 5)

```

![image-20200221135403039](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20200221135403039.png)

### **NMDS分析**

最好使用丰度表 而不是距离矩阵

```R
library(vegan)	#排序分析
#使用OTU丰度表作为输入文件还是现有的距离矩阵作为输入文件，NMDS排序的结果是不一致的。
#使用OTU丰度表作为输入文件的NMDS排序分析能够更充分地进行信息挖掘
rm(list = ls())
library(GUniFrac)
setwd("./Abundance/Nr_abundance/Distance/")
library(ggplot2)	#作图
library(ape)	#可选，用于读取进化树文件，见最后
##############################
##NMDS 排序（基于 OTU 丰度表）
#读入 OTU 丰度表
otu <- read.delim('Genu_0.001.txt', row.names = 1, sep = '\t', stringsAsFactors = FALSE, check.names = FALSE)
otu <- data.frame(t(otu))
distance <- vegdist(otu, method = 'bray')
distance <- as.matrix(distance)
write.table(distance, 'bray-curtis_distance.txt', col.names = NA, sep = '\t', quote = FALSE)
#排序，预设 4 个排序轴
nmds1 <- metaMDS(otu, distance = 'bray', k = 2)
?metaMDS()
#可简要查看结果
nmds1
#或
summary(nmds1)

#提取应力函数值（stress）
nmds1.stress <- nmds1$stress
#提取样本排序坐标
nmds1.point <- data.frame(nmds1$point)
#提取物种（OTU）排序坐标
nmds1.species <- data.frame(nmds1$species)

#可选择将结果输出并保存在本地，例如将样本坐标输出为 csv 格式
write.csv(nmds1.point, 'nmds.sample.csv')

#简要绘图展示
nmds_plot <- nmds1
nmds_plot$species <- {nmds_plot$species}[1:10, ]
plot(nmds_plot, type = 't', main = paste('Stress =', round(nmds1$stress, 4)))

##############################
##NMDS 排序（基于现有的距离矩阵）
#读入现有的距离矩阵
dis <- read.delim('bray-curtis_distance.txt', row.names = 1, sep = '\t', stringsAsFactors = FALSE, check.names = FALSE)
#排序，预设 4 个排序轴 dis为读入的距离矩阵
nmds2 <- metaMDS(as.dist(dis), k = 2)
#可简要查看结果
summary(nmds2)
#可发现“species”信息中为“1”，打印出来为“NA”
nmds2$species
#简要绘图展示，可发现与基于 OTU 丰度表的 NMDS 排序结果不同
plot(nmds2, type = 't', main = paste('Stress =', round(nmds2$stress, 4)))
#可使用 wascores() 将 OTU “被动地”加入 NMDS 排序图
species <- wascores(nmds2$points, otu)
text(species[1:10, ], rownames(species)[1:10], col = 'red')

##############################
##NMDS 评估  可以通过比较NMDS排序图内对象的距离与原始对象距离去评估NMDS结果（Shepard图）。若R2越大，则NMDS结果越合理。
stressplot(nmds_plot, main = 'Shepard 图')
gof <- goodness(nmds_plot)
plot(nmds_plot,type = 't', main = '拟合度')
points(nmds_plot, display = 'sites', cex = gof * 200, col = 'red')

##############################
##ggplot2 作图（使用基于 OTU 丰度表的 NMDS 排序结果，预设 2 个排序轴）
#读入 OTU 丰度表
otu <- read.delim('Genu_0.001.txt', row.names = 1, sep = '\t', stringsAsFactors = FALSE, check.names = FALSE)
otu <- data.frame(t(otu))

#读入样本分组文件
group <- read.delim('sample.txt', sep = '\t', stringsAsFactors = FALSE)

#排序，预设 2 个排序轴
nmds1 <- metaMDS(otu, distance = 'bray', k = 2)

#提取样本点坐标（前两轴）
sample_site <- nmds1.point[1:2]
sample_site$names <- rownames(sample_site)
names(sample_site)[1:2] <- c('NMDS1', 'NMDS2')


#为样本点坐标添加分组信息
sample_site <- merge(sample_site, group, by = 'names', all.x = TRUE)

#使用 ggplot2 绘制 NMDS 排序图
nmds_plot <- ggplot(sample_site, aes(NMDS1, NMDS2, group = group,color=group)) +
  geom_point(aes(color = group), size = 1.5, alpha = 0.8) + #可在这里修改点的透明度、大小
  scale_shape_manual(values = c(17, 16)) + #可在这里修改点的形状
  stat_ellipse(level = 0.95, show.legend = F) +
  scale_color_manual(values = c('red', 'blue')) + #可在这里修改点的颜色
  theme(panel.grid = element_blank(), panel.background = element_rect(color = 'black', fill = 'transparent')) + #去掉背景框
  theme(legend.key = element_rect(fill = 'transparent'), legend.title = element_blank()) + #去掉图例标题及标签背景
  labs(x = 'NMDS axis1', y = 'NMDS axis2', title = paste('Stress =', round(nmds1$stress, 4))) +
  theme(plot.title = element_text(hjust = 0.5))+ #标题居中
  annotate('text', label = 'Diarrhea', x = -0.3, y = 0.06, size = 5, colour = 'red') +
  annotate('text', label = 'Healthy', x = 0.25, y = 0.25, size = 5, colour = 'blue') 
  #geom_text(aes(label = group), data = species_site, color = 'green4', size = 2) #添加物种排序（top10 OTU，展示为标签）
nmds_plot
#ggsave('Genu_nmds.pdf', nmds_plot, width = 6, height = 5)
ggsave('NMDS.png', nmds_plot, width = 6, height = 5)

##############################
##基于 Weighted UniFrac 距离的 NMDS 排序（测试）

#加载函数


#OTU 丰度表
otu <- read.delim('test_otu.txt', row.names = 1, sep = '\t', stringsAsFactors = FALSE, check.names = FALSE)
otu <- data.frame(t(otu))

#进化树（使用 ape 包中的 read.tree() 读取，此处进化树必须为有根树）
otu_tree <- read.tree('test_tree.tre')

#距离矩阵（weighted unifrac 距离矩阵，使用 GUniFrac 包中的命令计算）
unifrac <- GUniFrac(otu, otu_tree)$unifracs
wei_unif_dis <- as.dist(unifrac[, , 'd_1']) #加权
unwei_unif_dis <- as.dist(unifrac[, , 'd_UW']) #非加权
#以非加权为例矩阵形式保存
unwei_unif_dis <- as.matrix(unwei_unif_dis)
write.table(unwei_unif_dis,'unweighted-unifrac_distance.txt',col.names = NA , sep = '\t',quote=FALSE)

#分组文件
group <- read.delim('test_group.txt', sep = '\t', stringsAsFactors = FALSE)

#NMDS 排序（基于 OTU 丰度表和进化树文件，使用 weighted unifrac 距离）
nmds_un1 <- metaMDS2(wei_unif_dis)
plot(nmds_un1, type = 't', main = paste('Stress =', round(nmds2$stress, 4)))
#NMDS 排序（直接基于 unweighted unifrac 距离矩阵）
nmds_un2 <- metaMDS(unwei_unif_dis)

#分别作图展示
sample_site2<- data.frame(nmds_un1$point); sample_site2 <- data.frame(nmds_un2$point)
sample_site1$names <- rownames(sample_site1); sample_site2$names <- rownames(sample_site2)
names(sample_site1)[1:2] <- c('NMDS1', 'NMDS2'); names(sample_site2)[1:2] <- c('NMDS1', 'NMDS2')
sample_site1 <- merge(sample_site1, group, by = 'names', all.x = TRUE); sample_site2 <- merge(sample_site2, group, by = 'names', all.x = TRUE)

ggplot(sample_site1, aes(NMDS1, NMDS2, color = group)) + geom_point()
ggplot(sample_site2, aes(NMDS1, NMDS2, color = group)) + geom_point()

```



**nmds实战例子（基于OTU表）**



```R
#载入分析包
library(vegan)
#载入分析数据
otu <- read.table("otu.txt",header=T,row.names=1,sep="\t")
#对分析数据进行转置
otu <- t(otu)
#进行NMDS分析
nmds <- metaMDS(otu)
#保存stress结果
capture.output(nmds,file = "Stress.txt" )
#保存scores结果
nmds_scores=scores(nmds, choices = c(1,2))
write.table(nmds_scores,file="NMDS_scores.txt")

scores <-  read.table("NMDS_scores.txt",header=T)
groups <- read.table("group1.txt", head=F,sep = "\t",colClasses = c("character"))
pich=c(21:24)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442")
Palette <- c("#000000","#000000","#000000","#000000")
plotdata <- data.frame(rownames(scores),scores$NMDS1,scores$NMDS2,groups$V2)
colnames(plotdata)=c("sample","MDS1","MDS2","group")
plotdata$sample <- factor(plotdata$sample)
plotdata$MDS1 <- as.numeric(as.vector(plotdata$MDS1))
plotdata$MDS2 <- as.numeric(as.vector(plotdata$MDS2))

library(ggplot2)
ggplot(plotdata, aes(MDS1, MDS2)) +
  geom_point(aes(colour=group,shape=group,fill=group),size=10)+
  scale_shape_manual(values=pich)+
  scale_colour_manual(values=Palette)+
  scale_fill_manual(values=cbbPalette)+
  labs(title="NMDS Plot") + xlab(paste("MDS1")) + ylab(paste("MDS2"))+
  theme(text=element_text(size=18))+
  geom_vline(aes(xintercept = 0),linetype="dotted")+
  geom_hline(aes(yintercept = 0),linetype="dotted")+
  geom_text(aes(x=max(MDS1),y=max(MDS2)),hjust=1,vjust=0,size=8,label=paste("Stress = ",round(nmds$stress,3),sep = ""),colour="black")+
  theme(panel.background = element_rect(fill='white', colour='black'), panel.grid=element_blank(), 
        axis.title = element_text(color='black',size=18),axis.ticks.length = unit(0.4,"lines"), 
        axis.ticks = element_line(color='black'),axis.line = element_line(colour = "black"), 
        axis.title.x=element_text(colour='black', size=18),axis.title.y=element_text(colour='black', size=18),axis.text=element_text(colour='black',size=18),
        legend.title=element_blank(),legend.text=element_text(family="Arial", size=18),legend.key=element_blank())+
  theme(plot.title = element_text(size=20,colour = "black",face = "bold"))

```



### aPCOA分析

```R
#用法
aPCoA(formula,data,maincov,drawEllipse=TRUE,drawCenter=TRUE)

#formula：Y~ A,Y为不像似性距离。
#data: 需要包含所有的协变量
#maincov： 研究的目标协变量，必须是因子
#drawEllipse: 95%置信椭圆
#drawCenter：中心点和其他点连线

#实例
library(mvabund)
library(vegan)
#install.packages("aPCoA")
library(aPCoA)
data("Tasmania")
data<-data.frame(treatment=Tasmania$treatment,block=Tasmania$block)
data

       treatment block
B1D1   Disturbed     1
B1D2   Disturbed     1

bray<-vegdist(Tasmania$abund, method="bray")
rownames(data)<-rownames(as.matrix(bray))
opar<-par(mfrow=c(1,2),
          mar=c(3.1, 3.1, 3.1, 5.1),
          mgp=c(2, 0.5, 0),
          oma=c(0, 0, 0, 4))
#treatment为研究的目标协变量，排除其他协变量的影响
result<-aPCoA(bray~block,data,treatment)
par(opar)
```





### **PCOA分析**

[PCOA示例](https://mp.weixin.qq.com/s/aCyx_r81nRd6W2PHVvuSyw)

无论使用OTU丰度表作为输入文件还是现有的距离矩阵作为输入文件，PCoA排序的结果是完全一致的。

```R
library(vegan)	#排序分析
#无论使用OTU丰度表作为输入文件还是现有的距离矩阵作为输入文件，PCoA排序的结果是完全一致的。
rm(list = ls())

library(ggplot2)	#作图
library(ape)
library(plyr)
#OTU 丰度表
otu <- read.delim('Genu_0.001.txt', row.names = 1, sep = '\t', stringsAsFactors = FALSE, check.names = FALSE)
otu <- data.frame(t(otu))
#或者现有的距离矩阵
dis <- read.delim('bray-curtis_distance.txt', row.names = 1, sep = '\t', stringsAsFactors = FALSE, check.names = FALSE)
#样本分组文件
group <- read.delim('sample.txt', sep = '\t', stringsAsFactors = FALSE)

#物种数据 Hellinger 预转化（处理包含很多 0 值的群落物种数据时，推荐使用）
otu<- decostand(otu, method = 'hellinger')

#排序（基于 OTU 丰度表）
distance <- vegdist(otu, method = 'bray')
bray <- as.matrix(bray_dis)
write.table(bray, 'bray-curtis_distance.txt', col.names = NA, sep = '\t', quote = FALSE)
pcoa <- cmdscale(distance, k = (nrow(otu) - 1), eig = TRUE)
#或者（基于现有的距离矩阵）
pcoa <- cmdscale(as.dist(dis), k = (nrow(dis) - 1), eig = TRUE)

#对于上述计算得到的样本间距离 distance，可转换为矩阵格式后输出，例如输出为 csv 格式
write.csv(as.matrix(distance), 'distance.csv', quote = F)

#使用vegan内置命令ordiplot()简要做图展示
ordiplot(scores(pcoa)[ ,c(1, 2)], type = 't')
#或者查看排序简要
summary(pcoa)
#查看主要排序轴的特征值和各样本在各排序轴中的坐标值
pcoa$eig
point <- data.frame(pcoa$point)

#可将样本坐标转化为数据框后导出，例如导出为 csv 格式
write.csv(point, 'pcoa.sample.csv')
#可使用 wascores() 计算物种坐标
species <- wascores(pcoa$points[,1:2], otu)

#可将物种坐标转化为数据框后导出，例如导出为 csv 格式
write.csv(species, 'pcoa.otu.csv')

#使用ggplot2画图
#坐标轴解释量（前两轴）
pcoa_eig <- (pcoa$eig)[1:2] / sum(pcoa$eig)

#提取样本点坐标（前两轴）
sample_site <- data.frame({pcoa$point})[1:2]
sample_site$names <- rownames(sample_site)
names(sample_site)[1:2] <- c('PCoA1', 'PCoA2')

#为样本点坐标添加分组信息
sample_site <- merge(sample_site, group, by = 'names', all.x = TRUE)

#可选输出，例如输出为 csv 格式
write.csv(sample_site, 'sample_site.csv', quote = F)

sample_site$group <- factor(sample_site$group, levels = c('D', 'H'))
##ddply() 计算同分组样本排序坐标顶点
group_border <- ddply(sample_site, 'group', function(df) df[chull(df[[2]], df[[3]]), ])##df[[2]]、df[[3]]代表第二、三列，即PCoA1和PCoA2坐标列
#绘图
pcoa_plot <- ggplot(sample_site, aes(PCoA1, PCoA2, group = group,color=group)) +
  theme(panel.grid = element_line(color = 'gray', linetype = 2, size = 0.1), panel.background = element_rect(color = 'black', fill = 'transparent'), legend.key = element_rect(fill = 'transparent')) + #去掉背景框
  geom_vline(xintercept = 0, color = 'gray', size = 0.4) + 
  #geom_polygon(data = group_border, aes(fill = group)) + #绘制多边形区域
  stat_ellipse(level = 0.95, show.legend = F) + #绘制置信椭圆
  geom_hline(yintercept = 0, color = 'gray', size = 0.4)+
  geom_point(aes(color = group), size = 2, alpha = 0.8)+#可在这里修改点的透明度、大小
 # scale_shape_manual(values = c(17, 16))+#可在这里修改点的形状
  scale_color_manual(values = c('red', 'blue')) + #可在这里修改点的颜色
  #theme(legend.position = 'none') + # 是否展示图例
  scale_fill_manual(values = c('#C673FF2E', '#73D5FF2E')) + #可在这里修改区块的颜色
  guides(fill = guide_legend(order = 1), shape = guide_legend(order = 2), color = guide_legend(order = 3)) + #设置图例展示顺序
  labs(x = paste('PCoA axis1: ', round(100 * pcoa_eig[1], 2), '%'), y = paste('PCoA axis2: ', round(100 * pcoa_eig[2], 2), '%')) +
annotate('text', label = 'Diarrhea', x = -0.19, y = 0.06, size = 5, colour = 'red') +
  annotate('text', label = 'Healthy', x = 0.1, y = 0.20, size = 5, colour = 'blue') 

pcoa_plot
ggsave('genu_pcoa.pdf', pcoa_plot, width = 6, height = 5)
```



**pcoa实战（基于OTU表）**

```R
setwd('./Script/Red_queen/PCOA/')
#载入绘图所需的包
library(vegan)
library(ape)
library(ggplot2)
library(ggrepel)

#分组数较少、组内生物学重复较多
data <- read.csv("otu.txt", head=TRUE,sep="\t",row.names = 1)
groups <- read.table("group1.txt",sep = "\t",header = F,colClasses = c("character"))
groups <- as.list(groups)
data <- t(data)
data[is.na(data)] <- 0
data <- vegdist(data,method = "bray")
pcoa<- pcoa(data, correction = "none", rn = NULL)
PC1 = pcoa$vectors[,1]
PC2 = pcoa$vectors[,2]
plotdata <- data.frame(rownames(pcoa$vectors),PC1,PC2,groups$V2)
colnames(plotdata) <-c("sample","PC1","PC2","group")
pich=c(21:24)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442")
Palette <- c("#000000", "#000000", "#000000", "#000000")
pc1 <-floor(pcoa$values$Relative_eig[1]*100)
pc2 <-floor(pcoa$values$Relative_eig[2]*100)

otu.adonis=adonis(data~V2,data = groups,distance = "bray")

ggplot(plotdata, aes(PC1, PC2)) +
  geom_point(aes(colour=group,shape=group,fill=group),size=12)+
  geom_text(aes(x = 0.05,y = 0.35,label = paste("PERMANOVA:\n    Group_A VS Group_B\n    p-value = ",otu.adonis$aov.tab$`Pr(>F)`[1],sep = "")),size = 10,hjust = 0)+
  stat_ellipse(aes(fill = group),geom = "polygon",level = 0.95,alpha = 0.3)+
  scale_shape_manual(values=pich)+
  scale_colour_manual(values=Palette)+
  scale_fill_manual(values=cbbPalette)+
  labs(title="PCoA - The composition of gut microbiome") + 
  xlab(paste("PC1 ( ",pc1,"%"," )",sep="")) + 
  ylab(paste("PC2 ( ",pc2,"%"," )",sep=""))+
  theme(text=element_text(size=30))+
  geom_vline(aes(xintercept = 0),linetype="dotted")+
  geom_hline(aes(yintercept = 0),linetype="dotted")+
  theme(panel.background = element_rect(fill='white', colour='black'),
        panel.grid=element_blank(), 
        axis.title = element_text(color='black',size=34),
        axis.ticks.length = unit(0.4,"lines"), axis.ticks = element_line(color='black'),
        axis.line = element_line(colour = "black"), 
        axis.title.x=element_text(colour='black', size=34),
        axis.title.y=element_text(colour='black', size=34),
        axis.text=element_text(colour='black',size=28),
        legend.title=element_blank(),
        legend.text=element_text(size=34),
        legend.key=element_blank(),legend.position = c(0.12,0.88),
        legend.background = element_rect(colour = "black"),
        legend.key.height=unit(1.6,"cm"))+
  theme(plot.title = element_text(size=34,colour = "black",hjust = 0.5,face = "bold"))

#同时显示两种分类方法
data <- read.csv("otu_taxa_table.txt", head=TRUE,sep="\t",row.names = 1)
groups <- read.table("group.list.txt",sep = "\t",header = F,colClasses = c("character"))
groups <- as.list(groups)
data <- t(data)
data[is.na(data)] <- 0
data <- vegdist(data,method = "bray")
pcoa<- pcoa(data, correction = "none", rn = NULL)
PC1 = pcoa$vectors[,1]
PC2 = pcoa$vectors[,2]
plotdata <- data.frame(rownames(pcoa$vectors),PC1,PC2,groups$V2,groups$V3)
colnames(plotdata) <-c("sample","PC1","PC2","Treatment","Time")
pich=c(21:25)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442")
pc1 <-floor(pcoa$values$Relative_eig[1]*100)
pc2 <-floor(pcoa$values$Relative_eig[2]*100)

ggplot(plotdata, aes(PC1, PC2)) +
  geom_point(aes(shape=Time,fill=Treatment,colour=Treatment),size=10)+ 
  scale_shape_manual(values=pich)+
  scale_colour_manual(values=cbbPalette)+
  scale_fill_manual(values=cbbPalette)+
  labs(title="PCoA - PC1-VS-PC2") + 
  xlab(paste("PC1 ( ",pc1,"%"," )",sep="")) + 
  ylab(paste("PC2 ( ",pc2,"%"," )",sep=""))+
  theme(text=element_text(size=30))+
  geom_vline(aes(xintercept = 0),linetype="dotted")+
  geom_hline(aes(yintercept = 0),linetype="dotted")+
  theme(panel.background = element_rect(fill='white', colour='black'),
        panel.grid=element_blank(), 
        axis.title = element_text(color='black',size=34),
        axis.ticks.length = unit(0.4,"lines"), axis.ticks = element_line(color='black'),
        axis.line = element_line(colour = "black"), 
        axis.title.x=element_text(colour='black', size=34),
        axis.title.y=element_text(colour='black', size=34),
        axis.text=element_text(colour='black',size=28),
        legend.title=element_text(colour = "black",size = 34,face = "bold"),
        legend.text=element_text(size=34),
        legend.key=element_blank(),
        legend.background = element_rect(colour = "black"),
        legend.key.height=unit(1.6,"cm"))+
  theme(plot.title = element_text(size=34,colour = "black",hjust = 0.5,face = "bold"))

#分组数目较多、组内生物学重复较少
data <- read.csv("otu 2.txt", head=TRUE,sep="\t",row.names = 1)
data <- t(data)
data[is.na(data)] <- 0
data <- vegdist(data)
pcoa<- pcoa(data, correction = "none", rn = NULL)
groups <- read.table("group 2.txt",sep = "\t",header = F,colClasses = c("character"))
groups <- as.list(groups)
groups$V2 <- factor(groups$V2,levels = c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"))
PC1 = pcoa$vectors[,1]
PC2 = pcoa$vectors[,2]
plotdata <- data.frame(rownames(pcoa$vectors),PC1,PC2,groups$V2)
colnames(plotdata) <-c("sample","PC1","PC2","group")
pc1 <-floor(pcoa$values$Relative_eig[1]*100)
pc2 <-floor(pcoa$values$Relative_eig[2]*100)
pich=rep(c(21:24),3)
library(RColorBrewer)
cbbPalette <- brewer.pal(12,"Set3")
Palette <- c(rep("#000000",12))

ggplot(plotdata, aes(PC1, PC2)) +
  geom_point(aes(colour=group,shape=group,fill=group),size=8)+
  geom_label_repel(aes(PC1,PC2,label = sample),fill = "white",color = "black",
                   box.padding = unit(0.6,"lines"),segment.colour = "grey50",
                   label.padding = unit(0.35,"lines")) +
  scale_shape_manual(values=pich)+
  scale_colour_manual(values=Palette)+
  scale_fill_manual(values=cbbPalette)+
  labs(title="PCoA - The composition of microbiome in sediment") + 
  xlab(paste("PC1 ( ",pc1,"%"," )",sep="")) + 
  ylab(paste("PC2 ( ",pc2,"%"," )",sep=""))+
  theme(text=element_text(size=30))+
  geom_vline(aes(xintercept = 0),linetype="dotted")+
  geom_hline(aes(yintercept = 0),linetype="dotted")+
  theme(panel.background = element_rect(fill='white', colour='black'),
        panel.grid=element_blank(), 
        axis.title = element_text(color='black',size=34),
        axis.ticks.length = unit(0.4,"lines"), axis.ticks = element_line(color='black'),
        axis.line = element_line(colour = "black"), 
        axis.title.x=element_text(colour='black', size=34),
        axis.title.y=element_text(colour='black', size=34),
        axis.text=element_text(colour='black',size=28),
        legend.title=element_blank(),
        legend.text=element_text(size=28),
        legend.key=element_blank(),legend.position = "right",
        legend.background = element_rect(colour = "black"),
        legend.key.height=unit(1.55,"cm"))+
  theme(plot.title = element_text(size=34,colour = "black",hjust = 0.5,face = "bold"))


```



### **PCA分析**

[提取PCA结果并利用ggplot2进行可视化](https://mp.weixin.qq.com/s/Adb_EJggkfa77Yjz7ZrjWw)

PCA（principal component analysis）中文名字叫主成分分析：基于特征向量的排序方法，分析对象是原始的定量数据。排序图展示样方之间的欧氏距离。

PCoA（principal coordinate analysis）中文名字叫主坐标分析：分析对象为距离矩阵，而非原始的样方-变量矩阵。因此可以灵活选着关联测度。

这样我们就发现了这两种非约束排序的区别：PCA是基于原始物种所做的排序分析，而PCoA则是基于物种组成计算得到的距离矩阵。

```R
library(tidyverse)
library(ggplot2)
library(ggsci)
library(Cairo)

# 进行PCA计算
pca = prcomp(iris[1:4])

# 提取每个样本对应的PCA坐标
pca.res = pca[["x"]] %>%
        as.data.frame()

# 将PCA结果和样本分组信息进行匹配
pca.res = cbind(pca.res, iris[,5])
colnames(pca.res)[5] = 'group'

# 计算每个主成分对方差的解释度
pca.var = pca$sdev^2 %>%
        as.data.frame()
pca.var$var = round(pca.var$. / sum(pca.var) * 100, 2) # 计算各主成分所占百分比
pca.var$pc = colnames(pca.res)[1:(ncol(pca.res)-1)]

# 绘制碎石图看每个主成分的解释量
ggplot(pca.var, aes(pc, var, fill = pc)) +
        geom_bar(stat = 'identity')+
        scale_fill_igv()+
        scale_y_continuous(expand = c(0,0)) +
        theme_bw() +
        labs(x = '主成分',
             y = '主成分解释量（%）')

# 进行PCA可视化
p = ggplot(pca.res, aes(PC1, PC2, color = group, shape = group))+ 
  # 选择X轴Y轴并映射颜色和形状
        geom_point(size = 3)+ # 画散点图并设置大小
        geom_hline(yintercept = 0,linetype="dashed") + # 添加横线
        geom_vline(xintercept = 0,linetype="dashed") + # 添加竖线
        scale_color_igv()+ # 设置颜色,此处为Integrative Genomics Viewer配色
        theme_bw() + # 加上边框
        stat_ellipse(level = 0.95)+ # 添加置信椭圆
        # 自动提取主成分解释度进行绘图
        labs(x = paste('PC1(', pca.var$var[1],'%)', sep = ''),
             y = paste('PC2(', pca.var$var[2],'%)', sep = '')) +
        theme(legend.position = c(0.85,0.85)) # 设置图例位置，此处为相对位置

p
# 保存成PDF格式
ggsave(p, filename = 'figures/PCA.pdf', 
       width = 5, height = 5, # 设置长款
       device = cairo_pdf, # 使用cairo输出设备
       family = 'Song') # 设置输出字体为宋体

**********一种老方法********
pca_plot = function(dddd,ggggg){
  library("FactoMineR")
  library("factoextra")
  df.pca <- PCA(t(dddd), graph = FALSE)
  fviz_pca_ind(df.pca,
               #axes = c(2,3),
               geom.ind = "point",
               col.ind = ggggg ,#分组
               addEllipses = TRUE,
               legend.title = "Groups"
  )
}
pca_plot(otu_file,group$group)
```

**3维PCA（pca3d包）**

```
install.packages('pca3d')
library(pca3d)
data("metabo"
    
pca <- prcomp( metabo[,-1], scale.= TRUE )
pca2d(pca,group = metabo[,1])
pca3d( pca, group= metabo[,1] )

# 黑色背景
pca3d( pca, group= metabo[,1],
       fancy= TRUE, 
       bg= "black",
       axes.color= "white", new= TRUE )

# 加个圆圈
pca3d( pca, group= metabo[,1],
       show.ellipses = T,
       #fancy= TRUE, 
       bg= "pink",
       axes.color= "white", new= TRUE )

```

![image-20201225191136241](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20201225191136241.png)





 **另一种PCA方法(基于ggbiplot包)**

```R
library(devtools)
install_github("vqv/ggbiplot") #安装
library(ggbiplot)
design <- read.table("sample.txt", header=T, row.names= 1, sep="\t")  #导入分组信息表 
count <- read.delim("Genu_0.001.txt", row.names= 1,  header=T, sep="\t") #导入相对丰度表
# 转换原始数据为百分比 norm = t(t(count)/colSums(count,na=T)) * 100
otu.pca <- prcomp(t(count), scale. = TRUE) 
ggbiplot(otu.pca, obs.scale = 1, var.scale = 1,groups = design$group, ellipse = TRUE,var.axes = F)
OTUs_mad.5 = head(count[order(apply(count,1,mad), decreasing=T),],n=8) 
otu.pca <- prcomp(t(OTUs_mad.5))
ggbiplot(otu.pca, obs.scale = 1, var.scale = 1, groups = design$group, ellipse = TRUE,var.axes = T)

# 我们仅用中值绝对偏差(mad)最大的8个OTUs进行主成分分析，即可将三组样品明显分开。
# 图中向量长短代表差异贡献，方向为与主成分的相关性。可以看到最长的向量Streptophyta与X轴近平行，
# 表示PC1的差异主要由此菌贡献。其它菌与其方向相反代表OTUs间可能负相关；夹角小于90%的代表两个OTUs有正相关

```



**另外基于ggord的PCA**

```R
library(ggord) #使用geom_ord_ellipse这个图层呢，随便你加置信区间，想加几个就加几个
#不仅针对LDA，也支持其它的ordination plot
devtools::install_github('fawda123/ggord')
library(ggord) 
library(MASS)
ord <- prcomp(iris[, -5])
p <- ggord(ord, iris$Species) ;p
library(yyplot) 
p + geom_ord_ellipse(ellipse_pro = .96, color='firebrick', size=1, lty=3) + 
  geom_ord_ellipse(ellipse_pro = .99, lty=2)  
```



**PCA**

```R
#载入分析包
library(FactoMineR)
library(ggrepel)
#载入绘图数据
df <- read.table("otu_taxa_table.txt",header = TRUE,row.names = 1,sep = "\t")
#载入分组数据
groups <- read.table("group.list.txt",header = FALSE,sep = "\t",
                     colClasses=c("character"))
#绝对丰度表转化为相对丰度表
df1 <- matrix(0,nrow = nrow(df), ncol = ncol(df))
for(i in 1:ncol(df)){
  df1[,i] = df[ ,i]/sum(df[ ,i])
}
colnames(df1) <- colnames(df)
rownames(df1) <- rownames(df)
df <- t(df1)

#PCA        
pca <- PCA(df[,1:ncol(df)],scale.unit = FALSE,graph = FALSE)
#PCA结果保存
write.csv(pca$ind$coord,file="pca.csv")


#支持一种分组方式和两种分组方式，
pc1 <- pca$ind$coord[,1]
pc2 <- pca$ind$coord[,2]
ncol <- ncol(groups)
group1 <- c()
group2 <- c() 
#通过循环式的分组文件中样本的顺序与pca结果文件中一致
for(i in 1:length(groups$V1)){
  Order <- grep(rownames(pca$ind$coord)[i],groups$V1)
  group1[i] <- groups$V2[Order]
  if(ncol==3){
    group2[i] <- groups$V3[Order]
  }}
#使用if来根据分组文件中的分组方式分别建立绘图文件
if(ncol==2){
  plotdata <- data.frame(rownames(pca$ind$coord),pc1,pc2,group1)
  colnames(plotdata) <- c("sample","PC1","PC2","group")
  point <- geom_point(aes(color = group,fill = group,shape = group),
                      size = 8)
}else if(ncol==3){
  plotdata <- data.frame(rownames(pca$ind$coord),pc1,pc2,group1,group2)
  colnames(plotdata) <- c("sample","PC1","PC2","group1","group2")
  point <- geom_point(aes(color = group1,fill = group1,shape = group2),
                      size = 8)
}

#通过向量指定绘图中的一些参数，包括坐标轴文字、样本点形状和颜色。
pc1 <- floor(pca$eig[1,2]*100)/100
pc2 <- floor(pca$eig[2,2]*100)/100
pich <- c(22:26)
cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442")


#计算与PCA中前两轴相关的物种
di <- dimdesc(pca, axes = c(1,2), proba = 0.05)
#根据相关性结果从大到小排序，每个轴选择排名前3的物种
dim1.cor <- pca$var$cor[row.names(pca$var$coord) %in% 
                          row.names(di$Dim.1$quanti[order(di$Dim.1$quanti[,2])[1:3],]),] 
dim2.cor <- pca$var$cor[row.names(pca$var$coord) %in% 
                          row.names(di$Dim.2$quanti[order(di$Dim.2$quanti[,2])[1:3],]),]
#建立绘图所需文件
dim12.cor <- rbind(dim1.cor,dim2.cor)
di.cor <- dim12.cor[,1:2]
di.cor <- as.data.frame(di.cor)

#载入绘图包
library(ggplot2)
library(ggrepel)
#图像绘制
p <- ggplot(plotdata,aes(PC1,PC2)) +
  #添加样本名称
  geom_label_repel(aes(PC1,PC2,label = sample),fill = "white",color = "black",
                   box.padding = unit(0.6,"lines"),segment.colour = "grey50",
                   label.padding = unit(0.35,"lines"),family = "Arial") + 
  point + 
  scale_shape_manual(values = pich)+
  scale_fill_manual(values=cbbPalette)+
  scale_color_manual(values=cbbPalette)+
  labs(title="PCA Plot") + 
  xlab(paste("PC1 ( ",pc1,"%"," )",sep="")) + 
  ylab(paste("PC2 ( ",pc2,"%"," )",sep=""))+
  #添加区分物种箭头
  geom_segment(data = di.cor,aes(x = 0,y = 0,xend = di.cor[,1]*0.06,
                                 yend = di.cor[,2]*0.06),
               arrow = arrow(angle = 40,length = unit(0.4,"cm")),size = 1.5) +
  #添加区分物种名称
  geom_label_repel(data = di.cor,aes(x = di.cor[,1]*0.06,y = di.cor[,2]*0.06,
                                     label = rownames(di.cor)),
                   fill = "grey50",color = "white",box.padding = unit(0.6,"lines"),
                   segment.colour = "grey50",label.padding = unit(0.35,"lines"),
                   family = "Arial") +
  theme(text=element_text(family = "Arial",size = 34))+
  geom_vline(aes(xintercept = 0),linetype = "dotted")+
  geom_hline(aes(yintercept = 0),linetype = "dotted")+
  theme(panel.background = element_rect(fill = 'white', colour = 'black'),
        panel.grid=element_blank(), 
        axis.title = element_text(color = "black",size = 34),
        axis.ticks.length = unit(0.4,"lines"), 
        axis.ticks = element_line(color = "black"),
        axis.line = element_line(colour = "black"), 
        axis.title.x=element_text(colour = "black", size = 34),
        axis.title.y=element_text(colour = "black", size = 34),
        axis.text=element_text(colour = "black",size=28),
        legend.title=element_text(colour = "black",size = 34,face = "bold"),
        legend.key=element_blank(),
        legend.background = element_rect(colour = "black"),
        legend.key.height=unit(1.2,"cm"),
        plot.title = element_text(size=34,colour = "black",hjust = 0.5,
                                  face = "bold"))
#保存为png图片
png(filename="PCA.png",res=700,height=5400,width=7200)
p
dev.off()
#保存为pdf格式
cairo_pdf(filename="PCA.pdf",height=10,width=10)
p
dev.off()

#通过ggsymbol包添加更多点样式
ggplot(plotdata, aes(PC1, PC2)) +
  geom_symbol(aes(symbol=group2,fill=group1,colour=group1),size=10)+ 
  scale_symbol_manual(values=pich)+
  scale_colour_manual(values=cbbPalette)+
  scale_fill_manual(values=cbbPalette)+
  geom_label_repel(aes(PC1,PC2,label = sample),fill = "white",color = "black",
                   box.padding = unit(0.6,"lines"),segment.colour = "grey50",
                   label.padding = unit(0.35,"lines"),family = "Arial") + 
  #添加区分物种箭头
  geom_segment(data = di.cor,aes(x = 0,y = 0,xend = di.cor[,1]*0.06,
                                 yend = di.cor[,2]*0.06),
               arrow = arrow(angle = 40,length = unit(0.4,"cm")),size = 1.5) +
  #添加区分物种名称
  geom_label_repel(data = di.cor,aes(x = di.cor[,1]*0.06,y = di.cor[,2]*0.06,
                                     label = rownames(di.cor)),
                   fill = "grey50",color = "white",box.padding = unit(0.6,"lines"),
                   segment.colour = "grey50",label.padding = unit(0.35,"lines"),
                   family = "Arial") +
  labs(title="PCoA - PC1-VS-PC2") + 
  xlab(paste("PC1 ( ",pc1,"%"," )",sep="")) + 
  ylab(paste("PC2 ( ",pc2,"%"," )",sep=""))+
  theme(text=element_text(size=30))+
  geom_vline(aes(xintercept = 0),linetype="dotted")+
  geom_hline(aes(yintercept = 0),linetype="dotted")+
  theme(panel.background = element_rect(fill='white', colour='black'),
        panel.grid=element_blank(),
        axis.title = element_text(color='black',size=34),
        axis.ticks.length = unit(0.4,"lines"), axis.ticks = element_line(color='black'),
        axis.line = element_line(colour = "black"),
        axis.title.x=element_text(colour='black', size=34),
        axis.title.y=element_text(colour='black', size=34),
        axis.text=element_text(colour='black',size=28),
        legend.title=element_text(colour = "black",size = 34,face = "bold"),
        legend.text=element_text(size=34),
        legend.key=element_blank(),
        legend.background = element_rect(colour = "black"),
        legend.key.height=unit(1.6,"cm"))+
  theme(plot.title = element_text(size=34,colour = "black",hjust = 0.5,face = "bold"))

```



### CA分析

对应分析（Correspondence Analysis，CA）在群落分析中常用于物种组成数据，排序结果展示的是样方间χ2距离。χ2距离不受双零问题的影响，适用于原始物种多度数据（和PCA相比，物种多度数据无需转化）。CA分析对象必须是频度或类频度、同量纲的非负数据，如物种个体计数或有-无数据。

## 相关性

[ppcor: 计算partial和semi-partial (part) correlations](https://cloud.tencent.com/developer/article/1635083)

他们利用了ppcor这个包计算环境因子之间的相关性。此文即是ppcor的学习笔记。



## 回归分析

[小白鱼笔记回归与建模](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzIxNzc1Mzk3NQ==&action=getalbum&album_id=1366876099606659074&scene=173&from_msgid=2247485098&from_itemidx=1&count=3&uin=&key=&devicetype=Windows+10+x64&version=63010043&lang=zh_CN&ascene=0&fontgear=2)

回归分析中所获取的ｒ是相关系数,Ｒ 是复相关系数，Ｒ2 是决定系数或相关指数。ｒ是简单相关系数，描述两变量间相关关系，或直线回归中反应变量与自变量之间的相关关系。在多元线性回归（多重线性回归）分析中，Ｒ 是复相关系数，回归平方和与总回归平方和之比的平方根，描述反应变量与多个自变量之间的线性相关关系。在多元线性回归（多重线性回归）分析中，Ｒ２ 是Ｒ 的平方，称决定系数或相关指数，是回归平方和在整个反应变量平方和（总回归平方和）中所占比重，一般Ｒ２ 愈大表示回归模型拟合愈好；但Ｒ２ 过大，意味着可能存在多重共线性问题，常用容忍度（１－Ｒ２）、方差膨胀因子（ＶＩＦ＝１／（１－Ｒ２））等指标判断，当容忍度＜0.1，或ＶＩＦ＞１０提示存在多重共线性问题。回归方程中ｒ是相关系数，Ｒ 是复相关系数，Ｒ２ 是复确定系数。在统计学的回归分析中虽然常用它们来描
述因变量与自变量的相关性和回归关系，但它们各自表达的统计学意义却不相同，因此三者不能混用。三者的关系是在一个因变量与一个自变量的线性相关和回归中，相关性符号用ｒ，回归关系符号用Ｒ２；而在一个因变量的非线性回归或一个因变量与多个自变量的线性相关和回归中，相关符号须用Ｒ，这时回归关系符号应该用Ｒ２。ｒ、Ｒ、Ｒ２ 使用错误，不但改变了使用符号的含意，同时也会使整个回归分析出现错误，

summary()函数查看线性模型信息，首先是有关线性模型数据拟合的残差应该是均值为0并且为正态分布（越小越好），对每个多元线性回归方程的系数，R显示他的估计值和标准误差(这些系数变化程度的估计)通常使用t检验来验证这些假设,R计算t值，定义为估计系数值与其标准误差的比。另一个R2表明模型和数据的吻合程度，即模型所能解释的数据变差的比例，越接近于1，模型拟合的越好。

#### 计算置信区间

```
out <- lm()
## broom::tidy(out)
out %>% tidy(conf.int = TRUE) ## 显示置信区间
out %>%
  tidy(conf.int = TRUE) %>%
  filter(!term %in% c("(Intercept)")) %>%
  ggplot(aes(
    x = reorder(term, estimate),
    y = estimate, ymin = conf.low, ymax = conf.high
  )) +
  geom_pointrange() +
  coord_flip() +
  labs(x = "", y = "OLS Estimate")
```

![image-20220527150330114](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202205271504606.png)

augment属性,

会返回一个数据框，这个数据框是在原始数据框的基础上，增加了模型的拟合值（`.fitted`）, 拟合值的标准误（`.se.fit`）, 残差（`.resid`）等列。



### 绘制回归曲线并添加标记

[R-ggplot2 学术散点图绘制 (qq.com)](https://mp.weixin.qq.com/s/osOaAKJjC2QDAApwyIaTQw)

![image-20210730211649167](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image1/image-20210730211649167.png)

```R
reg <- lm(dewpoint ~ temp, data = chic)

g +
  geom_abline(intercept = coefficients(reg)[1],
              slope = coefficients(reg)[2],
              color = "darkorange2", size = 1.5) +
  labs(title = paste0("y = ", round(coefficients(reg)[2], 2),
                      " * x + ", round(coefficients(reg)[1], 2)))
```

![image-20210902154022522](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202205271500841.png)

[R绘图 | 气泡散点图+拟合曲线 (qq.com)](https://mp.weixin.qq.com/s/2F8_UfGd8VLwrEbP0jzZ1Q)

```R
rm(list = ls())
library(ggpubr)
library(ggprism)
library(paletteer)

plot_df = read.csv('plot_df.csv')

## 设置主题
rel_size <- 1
my_theme <- theme_prism(border = TRUE, 
                        base_size = 5) +
  theme(strip.text.x = element_text(size = rel(rel_size*2)),
        title = element_text(size = rel(rel_size*2)),
        legend.box.spacing = unit(1, "cm"),
        legend.text = element_text(size = rel(rel_size*1.5)),
        legend.title = element_text(size = rel(rel_size*0.5)),
        axis.text.y = element_text(size = rel(rel_size*2), angle = 0, vjust = 0.2),
        axis.text.x = element_text(size = rel(rel_size*1.6), angle = 45),
        panel.grid = element_line(color = "gray",
                                  size = 0.15,
                                  linetype = 2),
        panel.spacing = unit(1, "lines"),
        plot.caption = element_text(size = rel_size*8)) 


# 绘图
p <- ggplot(plot_df, aes(x = log10(mean_gdp_per_capita), y = mean_access_perc)) + 
  # coord_trans("log10") +
  geom_point(data = plot_df, aes(size = mean_death_perc, fill = continent), pch = 21) +
  geom_smooth(method = "loess") +
  scale_fill_paletteer_d("colorblindr::OkabeIto") +
  ggpubr::stat_cor(method = "spearman", 
                   aes(label = paste(..rr.label.., ..p.label.., sep = "~")), 
                   color = "red", geom = "label", label.x = 3.8, label.y = 5) +
  # facet_wrap(vars(continent),drop = TRUE) +
  scale_y_continuous(breaks = c(0, 25, 50, 75, 100)) + 
  my_theme +
  ggtitle("Access to clean energy is associated with GDP") +
  labs(x = "Log10 of Average GDP per capita",
       y = "Average % access to clean fuels/tech",
       fill = "Continent", 
       size = "Average % Death",
       caption = "Spearman correlation\nAveraged values over years 1990-2019\nData source: OurWorldInData.org")

p
ggsave(plot = p, filename = 'week1.pdf',width = 5,height = 4)
```

![image-20220424154947896](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202204241551405.png)





#### **方案一**

```R
#方案一
head(mtcars)
mtcars1 <- mtcars
mtcars2 <- tidyr::gather(mtcars1,"variable","value",2:11)

iris_groups<- group_by(mtcars2, variable)

df_summarise<- summarise(iris_groups, mean(mpg), mean(value))

lab = mean(mtcars$mpg)

#----我们使用的都是同样的y做相关,所以呢都是相同的
ggplot2::ggplot(mtcars2,aes(value,mpg, colour=variable)) +
  ggplot2::geom_point() +
  ggpubr::stat_cor(label.y=lab*1.1)+
  ggpubr::stat_regline_equation(label.y=lab) +
  facet_wrap(~variable, scales="free_x") +
  geom_smooth(aes(value,mpg, colour=variable), method=lm, se=T)+
  theme_grey()
```

![image-20210317143211884](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20210317143211884.png)

#### **方案二**

```R
#方案二
library(ggplot2)
data <- read.table(file = "count.txt", sep = "\t", header = T, row.names = 1, stringsAsFactors = F)
data_matrix <- log2(data + 1)

model <- lm(featurecounts ~ htseqcount, data_matrix)
r2 <- summary(model)$r.squared
r2 <- paste0("r^2=", r2)

p <- ggplot(data_matrix, aes(x = featurecounts, y = htseqcount)) +
  geom_point(colour = "grey60", size = 1) +
  theme_light() +
  stat_smooth(method = lm, se = FALSE) +
  annotate("text", label = r2, x = 3.5, y = 1)
p
```

#### **方案三**

```R
#方案三
library(ggplot2) #加载ggplot2包
library(dplyr) #加载dplyr包
library(ggpmisc) #加载ggpmisc包
#展示 使用Species为setosa的亚集
iris2 <- subset(iris,Species == "setosa")
head(iris2)

#绘制点图，添加回归线
p <- ggplot(iris2, aes(Sepal.Length, Sepal.Width)) +
  geom_point(color = "grey50",size = 3, alpha = 0.6)
#回归线
#添加回归曲线
p + stat_smooth(color = "skyblue", fill = "skyblue", method = "lm")
#连接点到线
p +
  stat_smooth(color = "skyblue", formula = y ~ x,fill = "skyblue", method = "lm")+
  stat_fit_deviations(formula = y ~ x, color = "skyblue")
#添加回归公式
p +
  stat_smooth(color = "skyblue", formula = y ~ x,fill = "skyblue", method = "lm") +
  stat_poly_eq(
    aes(label = paste(..eq.label.., ..adj.rr.label.., sep = '~~~~')),
    formula = y ~ x,  parse = TRUE,
    size = 5, #公式字体大小
    label.x = 0.1,  #位置 ，0-1之间的比例
    label.y = 0.95)
#添加方差列表
p +  ylim(2,5) +
  stat_smooth(color = "skyblue", formula = y ~ x,fill = "skyblue", method = "lm") +
  stat_poly_eq(
    aes(label = paste(..eq.label.., ..adj.rr.label.., sep = '~~~~')),
    formula = y ~ x,  parse = TRUE,size = 3,label.x = 0.1, label.y = 0.99) +
  stat_fit_tb(tb.type = 'fit.anova',
              label.y.npc = "top", label.x.npc = "left",
  )
#label.y.npc 为另一种调整位置的方式 ，用label.y可完全避免重叠
#如担心方差表和公示与图重叠，可以通过ggplot2 的 ylim和xlim适当调整，然后调整位置即可。
#细节优化一下
p +  ylim(2,5) +
  stat_smooth(color = "skyblue", formula = y ~ x,fill = "skyblue", method = "lm") +
  stat_poly_eq(
    aes(label = paste(..eq.label.., ..adj.rr.label.., sep = '~~~~')),
    formula = y ~ x,  parse = TRUE,size = 4,label.x = 0.1, label.y = 0.95) +
  stat_fit_tb(method = "lm",
              method.args = list(formula = y ~ x),
              tb.type = "fit.anova",
              tb.vars = c(Effect = "term",
                          "df",
                          "M.S." = "meansq",
                          "italic(F)" = "statistic",
                          "italic(P)" = "p.value"),
              label.y = 0.87, label.x = 0.1,
              size = 4,
              parse = TRUE
  )
```

#### **实例**

```R
# 加载包
library(ggplot2)
library(ggpubr)
library(car)
# 加载iris数据集
data(iris)
aa<-iris
### 出图1
ggplot(aa,aes(x=aa$Sepal.Length,y=aa$Petal.Width))+#x,y参数
  geom_point(size=4,aes(color=aa$Species))+#点大小
  scale_color_manual(values=c("Tan4","OliveDrab3","DarkGreen"))+#颜色
  geom_smooth(method=lm,level=0.95,color="gray4")+
  stat_cor(method = "pearson",label.x.npc ="left",label.y.npc = 0.02)+
  labs(y="Sepal Width",x="Sepa Length")+#坐标轴
  theme(axis.title = element_text(color='black',size=9),#主题
        axis.ticks.length = unit(0.4,"lines"), axis.ticks = element_line(color='black'),
        axis.line = element_line(colour = "black"), 
        axis.title.x=element_text(colour='black', size=11),
        axis.title.y=element_text(colour='black', size=11),
        axis.text=element_text(colour='black',size=9),
        legend.title=element_blank(),
        legend.text=element_text(size=9),
        legend.key=element_blank(),
        legend.background = element_rect(colour = "White"))
### 出图2 
ggplot(aa,aes(x=aa$Sepal.Length,y=aa$Petal.Length))+
  geom_point(size=4,aes(color=aa$Species))+
  scale_color_manual(values=c("Tan4","OliveDrab3","DarkGreen"))+
  geom_smooth(method=lm,level=0.95,color="gray4")+
  stat_cor(method = "pearson",label.x.npc ="left",label.y.npc = 0.02)+
  theme_classic()+
  labs(y="Sepal Width",x="Sepa Length")+
  theme(axis.title = element_text(color='black',size=9),
        axis.ticks.length = unit(0.4,"lines"), axis.ticks = element_line(color='black'),
        axis.line = element_line(colour = "black"), 
        axis.title.x=element_text(colour='black', size=11),
        axis.title.y=element_text(colour='black', size=11),
        axis.text=element_text(colour='black',size=9),
        legend.title=element_blank(),
        legend.text=element_text(size=9),
        legend.key=element_blank(),
        legend.background = element_rect(colour = "White"))
```

![image-20210320140723521](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20210320140723521.png)

**多组绘制**

```R
#加载数据
# 加载包
library(ggplot2)
library(ggpubr)
library(car)
# 加载iris数据集
data(iris)
aa<-iris
### 出图1
library(ggplot2)
#查看物种
aa$Species

#画图
ggplot(aa,aes(x=aa$Sepal.Length,y=aa$Petal.Width))+#x,y参数
  geom_point(size=2.5,aes(color=aa$Species))+#点大小
  scale_color_manual(values=c("#6959CD","#4169B2","#8FBC8F"),limits=c("setosa","versicolor","virginica"))+#颜色
  geom_smooth(aes(color =Species), method = 'lm', se = TRUE, show.legend = FALSE) +
  #geom_smooth(method=lm,level=0.95,color="gray4")+
  #stat_cor(method = "pearson",label.x.npc ="left",label.y.npc = 0.02)+
  theme_bw()+labs(y="Sepal.Length",x="Petal.Width")+
  theme(axis.title = element_text(color='black',size=9),
        axis.ticks.length = unit(0.4,"lines"), axis.ticks = element_line(color='black'),
        axis.line = element_line(colour = "black"), 
        legend.title=element_blank(),
        legend.text=element_text(size=9),
        legend.key=element_blank(),
        legend.background = element_rect(colour = "White"))


#计算线性回归的R2和P值
#子数据集setosa
fit_setosa<-lm(Sepal.Length~Petal.Width, data = subset(aa, Species == 'setosa'))
summary(fit_setosa)
#R2=0.058 P>0.05
#子数据集versicolor
fit_versicolor<-lm(Sepal.Length~Petal.Width, data = subset(aa, Species == 'versicolor'))
summary(fit_versicolor)
#R2=0.284 P<0.001
#子数据集virginica
fit_virginica<-lm(Sepal.Length~Petal.Width, data = subset(aa, Species == 'virginica'))
summary(fit_virginica)
#R2=0.06 P<0.05

#将R2和P值标记在图上
ggplot(aa,aes(x=aa$Sepal.Length,y=aa$Petal.Width))+#x,y参数
  geom_point(size=2.5,aes(color=aa$Species))+#点大小
  scale_color_manual(values=c("#6959CD","#4169B2","#8FBC8F"),limits=c("setosa","versicolor","virginica"))+#颜色
  geom_smooth(aes(color =Species), method = 'lm', se = TRUE, show.legend = FALSE) +
  #geom_smooth(method=lm,level=0.95,color="gray4")+
  #stat_cor(method = "pearson",label.x.npc ="left",label.y.npc = 0.02)+
  theme_bw()+labs(y="Sepal.Length",x="Petal.Width")+
  theme(axis.text=element_text(colour='black',size=9))+
  annotate('text', label = 'R2=0.058; p>0.05', x =7, y = 0.3, size =3.5,color="#6959CD")+#子数据集setosa
  annotate('text', label = 'R2=0.284; p<0.001', x = 6, y = 0.8, size =3.5,color="#4169B2")+#子数据集versicolor
  annotate('text', label = 'R2=0.06; p<0.05', x =5, y = 2.3, size =3.5,color="#8FBC8F") #子数据集virginica

```

![image-20210421155127016](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20210421155127016.png)

### 多项式回归



```R
# 简单的线性回归 普通最小二乘法（Ordinary Least Square，OLS）回归法
fit <- lm(comm_sim~site_dis_km, data = comm_dis)
summary(fit)  #展示拟合模型的简单统计
#例如
coefficients(fit)[1]  #获取截距
coefficients(fit)[2]  #获取 site_dis_km 的斜率
 
#也可以 names(summary(fit)) 后查看主要的内容项，然后从中提取，例如
summary(fit)$adj.r.squared  #校正后 R2
#ggplot2 绘制带线性拟合线的散点图
#拟合线同时考虑了“黑点”和“红点”，其中“红点”代表了与 NADR 地区中采样地点有关的距离关系
library(ggplot2)
 
p <- ggplot(comm_dis, aes(site_dis_km, comm_sim)) +
geom_point(aes(color = as.character(edge))) +
scale_color_manual(values = c('red', 'black'), limit = c('0', '1')) +
geom_smooth(method = 'lm', se = FALSE) +
theme(panel.grid = element_blank(), panel.background = element_rect(color = 'black', fill = 'transparent'),
    legend.position = 'none', plot.title = element_text(hjust = 0.5)) +
scale_y_continuous(limit = c(0, 1)) +
labs(x = 'Distance (km)', y = 'Bray-curtis similarity', title = 'North')
 
#提取上述结果中重要的统计值，用于在图中将方程式标注出
label_fit <- data.frame(
    formula = sprintf('italic(Y) == %.7f*italic(X) + %.3f', coefficients(fit)[2], coefficients(fit)[1]),
    R2 = sprintf('italic(R^2) == %.3f', summary(fit)$adj.r.squared),
    p_value = sprintf('italic(P) < %.3f', 0.001))
 
p +
geom_text(x = 3000, y = 0.9, aes(label = formula), data = label_fit, parse = TRUE,
    hjust = 0, color = 'black', show.legend = FALSE) +
geom_text(x = 4000, y = 0.8, aes(label = R2), data = label_fit, parse = TRUE,
    hjust = 0, color = 'black', show.legend = FALSE) +
geom_text(x = 4000, y = 0.7, aes(label = p_value), data = label_fit, parse = TRUE,
    hjust = 0, color = 'black', show.legend = FALSE)

#多项式回归
#首先不妨先看一下简单一次线性回归的结果
fit1 <- lm(comm_sim~site_dis_km, data = comm_dis)
summary(fit1)  #展示拟合模型的简单统计，发现 R2 是很低的
#然后上述简单线性回归基础上添加个二次项
#I(X^2) 添加二次项，I(X^3) 可添加三次项，以此类推
fit2 <- lm(comm_sim~site_dis_km+I(site_dis_km^2), data = comm_dis)
summary(fit2)  #展示拟合模型的简单统计，相比一次线性回归有效提高了回归精度
#ggplot2 绘制带线性拟合线的散点图
library(ggplot2)
 
p <- ggplot(comm_dis, aes(x = site_dis_km, y = comm_sim)) +
geom_point(color = 'black') +
geom_smooth(method = 'lm', se = FALSE, color = 'blue', formula = y~x) +
geom_smooth(method = 'lm', se = FALSE, color = 'blue4', formula = y~poly(x, 2)) +
theme(panel.grid = element_blank(), panel.background = element_rect(color = 'black', fill = 'transparent'),
    legend.position = 'none', plot.title = element_text(hjust = 0.5)) +
scale_y_continuous(limit = c(0, 1)) +
labs(x = 'Distance (km)', y = 'Bray-curtis similarity', title = 'Both')
 
#这次就不标出具体的拟合式了，只标出简单一次和二次回归的 R2 比较
label_fit <- data.frame(
    R1_2 = sprintf('italic(R^2) == %.3f', summary(fit1)$adj.r.squared),
    R2_2 = sprintf('italic(R^2) == %.3f', summary(fit2)$adj.r.squared))
 
p +
geom_text(x = 10000, y = 0.9, aes(label = R1_2), data = label_fit, parse = TRUE,
    hjust = 0, color = 'blue', show.legend = FALSE) +
geom_text(x = 10000, y = 0.8, aes(label = R2_2), data = label_fit, parse = TRUE,
    hjust = 0, color = 'blue4', show.legend = FALSE)
```

## Mantel test

[R包vegan的Mantel tests ](http://blog.sciencenet.cn/home.php?mod=space&uid=3406804&do=blog&id=1209722)

[mantel test组合图初步实现方法](https://mp.weixin.qq.com/s/rAMG3_H7JgvClpKENYKtoQ)

![image-20210512162736302](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/image-20210512162736302.png)

Mantel tests是确定两组距离测度矩阵（而非两组变量矩阵）之间相关性的相关性测试非参数统计方法，用于判断一个矩阵中的样本距离与另一矩阵中的样本距离是否相关。Mantel tests零假设为响应变量矩阵中对象之间的距离与解释变量矩阵不存在相关，如果结果中p值显著，则拒绝零假设，即存在相关性，随着一个矩阵中样本之间距离的增加（或减少），另一矩阵中对应样本之间的距离也增加（或减少）。

此外，Mantel方法还可用于检验假设或模型。在这种模型测试方法中，一个矩阵包含响应数据，另一个矩阵代表了要测试的先验模型（检验的备择假设），如果找到了重要的Mantel统计信息，它们将为模型提供一些支持。

```R
library(vegan)
 
#读取上述数据集
df <- read.csv('Your_OTU_table.csv', header= TRUE)
 
##计算距离
#根据物种丰度数据，计算样方间的 Bray-curtis 距离
abund <- df[ ,8:ncol(df)]
dist.abund <- vegdist(abund, method = 'bray')
 
#根据环境测量指标，计算样方间的欧几里得距离
#这里只选择了其中的温度指标，期望关注物种变化与温度的相关性
temp <- df$Temperature
dist.temp <- dist(temp, method = 'euclidean')
 
#如果期望关注多种环境的协同作用，就选择一个环境子集，计算样方间的欧几里得距离
#例如使用 4 种环境数据，但此时需要执行数据标准化，以消除量纲差异
env <- df[ ,2:5]
scale.env <- scale(env, center = TRUE, scale = TRUE)
dist.env <- dist(scale.env, method = 'euclidean')
 
#根据经纬度，计算样方间实际的地理距离
geo <- data.frame(df$Longitude, df$Latitude)
d.geo <- distm(geo, fun = distHaversine)       #library(geosphere)
dist.geo <- as.dist(d.geo)
 
##执行 Mantel tests，详情 ?mantel，以下为 3 个示例
#物种丰度和温度的相关性，以 spearman 相关系数为例，9999 次置换检验显著性（Mantel tests 基于随机置换的方法获取 p 值）
abund_temp <- mantel(dist.abund, dist.temp, method = 'spearman', permutations = 9999, na.rm = TRUE)
abund_temp
 
#物种丰度和地理距离的相关性，以 spearman 相关系数为例，9999 次置换检验显著性
abund_geo <- mantel(dist.abund, dist.geo, method = 'spearman', permutations = 9999, na.rm = TRUE)
abund_geo
 
#物种丰度和 4 种环境组合的相关性，以 spearman 相关系数为例，9999 次置换检验显著性
abund_env <- mantel(dist.abund, dist.env, method = 'spearman', permutations = 9999, na.rm = TRUE)
abund_env

library(ggplot2)
 
#某物种与温度的相关性，横轴温度，纵轴物种丰度，颜色表示样方的纬度
xx = ggplot(df, aes(x = Temperature, y = Pelagibacteraceae.OTU_307744)) + 
    geom_smooth(method = 'lm', alpha = 0.2, colour = 'black') + 
    geom_point(aes(colour = Latitude), size = 4) +
    labs(y = 'Pelagibacteraceae (OTU 307744) (%)', x = 'Temperature (C)') + 
    theme( axis.text.x = element_text(face = 'bold',colour = 'black', size = 12), 
        axis.text.y = element_text(face = 'bold', size = 11, colour = 'black'), 
        axis.title= element_text(face = 'bold', size = 14, colour = 'black'), 
        panel.background = element_blank(), 
        panel.border = element_rect(fill = NA, colour = 'black'), 
        legend.title = element_text(size =12, face = 'bold', colour = 'black'),
        legend.text = element_text(size = 10, face = 'bold', colour = 'black')) +
    scale_colour_continuous(high = 'navy', low = 'salmon')
 
xx
#对于图中的线性回归
fit <- lm(df$Temperature~df$Pelagibacteraceae.OTU_307744)
summary(fit)
```

### **Partial Mantel Test**

Partial Mantel Test 被用来检测在控制矩阵 C 的情况下，A 和 B 矩阵的相关性，也就是在 Mantel test  测试的基础上添加了第三个“控制”矩阵。Partial Mantel test[21]在控制矩阵 C(如生态距离矩阵，即  pH，温度或者地理位置等的差异矩阵)的效应下，来检验矩阵 A(如细菌群落、基因或功能等距离矩阵等)的残留变异是否和矩阵 B(如  噬菌体群落、基因或功能等距离矩阵等)相关。一般只要能计算有距离属性的值，都可以转化为距离(如 Bray-Curtis  等)矩阵进行分析。该分析输入 2 个数值型矩阵，第三个控制矩阵可通过选择因子(如环境因子的表头)来确定 的。





## 机器学习

[总结系列|10种最常用机器学习算法的 Python 和 R 代码集合 (qq.com)](https://mp.weixin.qq.com/s/-2nhea7zbN9pldBvMd7yyA)

[ISLR tidymodels版本-Introduction to Statistical Learning](https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/index.html)

[ISLR第2版中文翻译](https://esl.hohoweiya.xyz/13-Prototype-Methods-and-Nearest-Neighbors/13.1-Introduction/index.html)

[【机器学习基础】详细介绍 7 种交叉验证方法！ (qq.com)](https://mp.weixin.qq.com/s/gbws_5hgk8ii-X-jh908og)

[#机器学习笔记 (qq.com)](https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=MzUzMTEwODk0Ng==&scene=1&album_id=1415246513353408512&count=3&scene=21#wechat_redirect)

[Quickstart | mlr3 book (mlr-org.com)](https://mlr3book.mlr-org.com/index.html)



基于《精通机器学习基于R》和《Machine Learning with R，Tidyverse and Mlr》两书学习机器学习中常用的几种方法

### Classification 

#### 基于K近邻法分类

- 理解偏差-方差权衡

- 过拟合和欠拟合

- 使用交叉验证评估模型表现
- 构建k-最近邻模型
- 调试超参数

标签数据，所以是监督学习算法，可以被使用分类和回归问题~ KNN处理分类问题的方法是找最近的点来确定正确的分类。

![image-20220425203323981](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202204252033036.png)













> 二、基于book2













#### 基于逻辑回归分类

优势：

- 可以处理分类变量和连续变量
- 模型参数容易解释
- 预测变量不比服从正态分布

劣势

- 当类别完全分离时候不起作用
- 改模型假设是线性分割开的，换句话说，他假设数据是个n维度的平面，n表示分为几部分，如果一个曲线平面被用来分割这个类别，逻辑回归相比于其他算法就不太好了
- 他假设一个每个预测值和log odds直接是一个线性关系，比如，低的方案和高的预测值是一个类别，但是有着中等值的预测值属于另一个类别，这个线性关系将被打破

注意：

1. 逻辑斯蒂回归就是以对数发生比为相应变量进行线性拟合，，即log(P(Y)/1  P(Y)) =B0+B1x。这里的系数是通过极大似然估计得到的，而不是通过OLS。



**Code**:pig_nose:

------

> 一、 基于book1 

使用**biopsy**数据集

```R
#### 解决分类问题
library(MASS)
data(biopsy)
str(biopsy)
biopsy$ID = NULL
names(biopsy) = c("thick", "u.size", "u.shape", "adhsn", 
                  "s.size", "nucl", "chrom", "n.nuc", "mit", "class")
names(biopsy)
biopsy.v2 <- na.omit(biopsy)
y <- ifelse(biopsy.v2$class == "malignant", 1, 0) # 结果变量可能要求是数值型的
```

使用ggplot2进行可视化-箱线图观察数据分布

```R
library(reshape2)
library(ggplot2)
biop.m <- melt(biopsy.v2, id.var = "class")
ggplot(data = biop.m, aes(x = class, y = value)) + 
  geom_boxplot() +
  facet_wrap(~ variable, ncol = 3)
```

![image-20220504223328711](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202205042233002.png)

从中位数和间隔距离和相关分布，可以看出nuclei是一个重要特征，所有特征都是定量的，我们可以进行相关性分析，逻辑斯蒂回归的共线性也可能发生偏离

```
# 检查共线性
library(corrplot)
bc <- cor(biopsy.v2[ ,1:9]) #create an object of the features
corrplot.mixed(bc)
```

![image-20220504223522151](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202205042235312.png)

从相关性可以看出 有共线性的问题，进行VIF分析检测，从原始数据建立两个不同数据集的目的在于，可以更加准确的预测未使用的数据和位置数据。

我们首先划分数据集为训练集和测试集：50/50 60/40 70/30

```R
## 按照70/30的比例划分数据
set.seed(123) #random number generator
ind <- sample(2, nrow(biopsy.v2), replace = TRUE, prob = c(0.7, 0.3))
train <- biopsy.v2[ind==1, ] #the training data set
test <- biopsy.v2[ind==2, ] #the test data set
str(test) #confirm it worked
table(train$class)
table(test$class)
> table(train$class)
benign malignant
302 172
> table(test$class)
benign malignant
142 67
```

构建逻辑斯蒂回归模型，使用R中glm()函数中的family = binomial这个参数

```R
## 逻辑斯蒂回归 使用binomial参数
full.fit <- glm(class ~ ., family = binomial, data = train)
summary(full.fit)

Call:
glm(formula = class ~ ., family = binomial, data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.3397  -0.1387  -0.0716   0.0321   2.3559  

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  -9.4293     1.2273  -7.683 1.55e-14 ***
thick         0.5252     0.1601   3.280 0.001039 ** 
u.size       -0.1045     0.2446  -0.427 0.669165    
u.shape       0.2798     0.2526   1.108 0.268044    
adhsn         0.3086     0.1738   1.776 0.075722 .  
s.size        0.2866     0.2074   1.382 0.167021    
nucl          0.4057     0.1213   3.344 0.000826 ***
chrom         0.2737     0.2174   1.259 0.208006    
n.nuc         0.2244     0.1373   1.635 0.102126    
mit           0.4296     0.3393   1.266 0.205402    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 620.989  on 473  degrees of freedom
Residual deviance:  78.373  on 464  degrees of freedom
AIC: 98.373

Number of Fisher Scoring iterations: 8
 
 
 ## 使用confint（）函数可以对模型进行95%的置信区间的检验
  confint(full.fit)
 Waiting for profiling to be done...
                   2.5 %     97.5 %
(Intercept) -12.23786660 -7.3421509
thick         0.23250518  0.8712407
u.size       -0.56108960  0.4212527
u.shape      -0.24551513  0.7725505
adhsn        -0.02257952  0.6760586
s.size       -0.11769714  0.7024139
```

注意 两个显著特征的置信区间不包括0，对于逻辑斯蒂回归的参数，不能解释当X改变1个单位时候Y改变多少，通过exponent(beta)指数函数转化为优势比odds ratios, 优势比可以解释为特征中1个单位的变化导致的结果发生比的变化，系数大于1 说明当特征值增加时候，结果的发生比增加，反之。

```
> exp(coef(full.fit))
 (Intercept)        thick       u.size      u.shape        adhsn 
8.033466e-05 1.690879e+00 9.007478e-01 1.322844e+00 1.361533e+00 
      s.size         nucl        chrom        n.nuc          mit 
1.331940e+00 1.500309e+00 1.314783e+00 1.251551e+00 1.536709e+00 
```

计算VIF统计量判断潜在的多重共线性

```
> vif(full.fit)
   thick   u.size  u.shape    adhsn   s.size     nucl    chrom 
1.235204 3.248811 2.830353 1.302178 1.635668 1.372931 1.523493 
   n.nuc      mit 
1.343145 1.059707 
```

没有共线性，接下来建立一个响量，表示预测概率

```
> ##建立向量表示预测概率
> train.probs <- predict(full.fit, type = "response")
> train.probs[1:5] #inspect the first 5 predicted probabilities
         1          3          6          7          9 
0.02052820 0.01087838 0.99992668 0.08987453 0.01379266 
> contrasts(train$class) ## 查看哑变量
```

下一步需要评价模型在训练集上的执行的效果，在评价测试集上的拟合程度，通过生成混淆矩阵，car包或者InformationValue包生虫，0 1 来表示结果。函数区别良性结果和恶性结果使用的默认值是0.50，也就是说，当概率大于等于0.50时，就认为这个结果是恶性的：

```R
> library(InformationValue)
Warning message:
程辑包‘InformationValue’是用R版本4.1.3 来建造的 
> trainY <- y[ind==1]
> testY <- y[ind==2]
> ## 评价它在测试集上的拟合程度，生成一个和混合矩阵
> confusionMatrix(trainY, train.probs)
    0   1
0 294   7
1   8 165
> # optimalCutoff(trainY, train.probs)
> misClassError(trainY, train.probs)
[1] 0.0316
```

训练集上只有3.16%的预测错误率，接下来预测测试集

```R
> test.probs <- predict(full.fit, newdata = test, type = "response")
> misClassError(testY, test.probs)
[1] 0.0239
> confusionMatrix(testY, test.probs)
    0  1
0 139  2
1   3 65
```

大概有98%的预测正确率，我们需要使用交叉验证来提供测试集上的预测正确率，以及近可能的避免过拟合，K折交叉验证或者留一交叉验证方法，LOOCV可以获得近乎无偏的估计但是会有很高的方差，所以大多数需要将K设置为5或10

使用bestglm包，依赖于leaps包，交叉验证的语法和数据格式比较严格, 我们需要将结果变量编码为0或者1，如果是因子 不起作用，而且使用这个包结果因子必须是最后一列，而且要删除所有无用的列

```R
> library(bestglm)
载入需要的程辑包：leaps
Warning message:
程辑包‘bestglm’是用R版本4.1.3 来建造的 
> #使用这个
> #程序包的另外一个要求是结果变量（或y）必须是最后一列，而且要删除所有没有用的列
> X <- train[, 1:9]
> Xy <- data.frame(cbind(X, trainY))
> ## IC使用交叉验证  HTF方法是K折交叉验证  REP告诉只迭代一次
> bestglm(Xy = Xy, IC = "CV", CVArgs = list(Method = "HTF", K = 10, REP = 1), 
+         family=binomial)
Morgan-Tatar search since family is non-gaussian.
CV(K = 10, REP = 1)
BICq equivalent for q in (7.16797006619085e-05, 0.273173435514231)
Best Model:
              Estimate Std. Error   z value     Pr(>|z|)
(Intercept) -7.8147191 0.90996494 -8.587934 8.854687e-18
thick        0.6188466 0.14713075  4.206100 2.598159e-05
u.size       0.6582015 0.15295415  4.303260 1.683031e-05
nucl         0.5725902 0.09922549  5.770596 7.899178e-09
```

Xy使我们已经格式化的数据，IC 使用的信息准则为交叉验证，CVArgs使我们要使用的交叉验证参数，HTF方法就是K验证，REP 是告诉程序随机使用等分只迭代1次。 指定family = gaussian，就可以进行线性回归，由此我们得到可三个最好的特征 thick  、u.size 和 nucl

再次逻辑回归看下表现，predict函数不能用于bestglm生成的模型

```R
## 得到特征后放入glm函数中
reduce.fit <- glm(class ~ thick + u.size + nucl, family = binomial, data = train)
## 测试集预测
test.cv.probs = predict(reduce.fit, newdata = test, type = "response")
misClassError(testY, test.cv.probs)
confusionMatrix(testY, test.cv.probs)
```

精确度有下降，使用BIC最优子集法试试

```R
## 使用BIC的最优子集方法
> bestglm(Xy = Xy, IC = "BIC", family = binomial)
Morgan-Tatar search since family is non-gaussian.
BIC
BICq equivalent for q in (0.273173435514231, 0.577036596263757)
Best Model:
              Estimate Std. Error   z value     Pr(>|z|)
(Intercept) -8.6169613 1.03155250 -8.353391 6.633065e-17
thick        0.7113613 0.14751510  4.822295 1.419160e-06
adhsn        0.4537948 0.15034294  3.018398 2.541153e-03
nucl         0.5579922 0.09848156  5.665956 1.462068e-08
n.nuc        0.4290854 0.11845720  3.622282 2.920152e-04
> bic.fit <- glm(class ~ thick + adhsn + nucl + n.nuc, 
+                family = binomial, data = train)
> test.bic.probs = predict(bic.fit, newdata = test, type = "response")
> misClassError(testY, test.bic.probs)
[1] 0.0239
> confusionMatrix(testY, test.bic.probs)
    0  1
0 138  1
1   4 66
```

至于哪个模型更好，在任何正常的情况下，如果具有相同的泛化效果，经验法则会选择最简单或者解释性最好的模型。



> Code- from book2

加载数据集

```R
#          SOURCE CODE FOR CHAPTER 4          #
###############################################

# LOAD PACKLAGES ----
library(mlr)

library(tidyverse)
detach("package:stats")

# LOAD DATA ----
#install.packages("titanic")

data(titanic_train, package = "titanic")

titanicTib <- as_tibble(titanic_train)

titanicTib

# CLEAN DATA ----
fctrs <- c("Survived", "Sex", "Pclass")
## 指定列变为因子水平
titanicClean <- titanicTib %>%
  mutate_at(.vars = fctrs, .funs = factor) %>%
  mutate(FamSize = SibSp + Parch) %>%
  select(Survived, Pclass, Sex, Age, Fare, FamSize)

titanicClean
glimpse(titanicClean)
```

绘图展示水平的分类情况

```R
# PLOT DATA ----
titanicUntidy <- gather(titanicClean, key = "Variable", value = "Value", 
                        -Survived)
titanicUntidy 

titanicUntidy %>%
  filter(Variable != "Pclass" & Variable != "Sex") %>%
  ggplot(aes(Survived, as.numeric(Value))) +
  facet_wrap(~ Variable, scales = "free_y") +
  ## 绘制分位数
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  theme_bw()

titanicUntidy %>%
  filter(Variable == "Pclass" | Variable =="Sex") %>%
  ggplot(aes(Value, fill = Survived)) +
  facet_wrap(~ Variable, scales = "free_x") +
  geom_bar(position = "fill") +
  theme_bw()
```

![image-20220507093358400](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202205070934006.png)

构建逻辑斯蒂回归模型

```R
# CREATE TASK AND LEARNER, AND ATTEMPT TO TRAIN MODEL ----
titanicTask <- makeClassifTask(data = titanicClean, target = "Survived")

logReg <- makeLearner("classif.logreg", predict.type = "prob")
## 不支持数据中含有NA
logRegModel <- train(logReg, titanicTask)
# 数据中有NA值报错

# COUNT MISSING VALUES IN Age VARIABLE ----
titanicClean$Age

sum(is.na(titanicClean$Age))

# IMPUTE MISSING VALUES ----

## 处理缺失值利用平均值代替NA
mean(titanicClean$Age,na.rm =TRUE) # 29.69912
imp <- impute(titanicClean, cols = list(Age = imputeMean()))

sum(is.na(titanicClean$Age))

sum(is.na(imp$data$Age))

##使用新数据训练模型
titanicTask <- makeClassifTask(data = imp$data, target = "Survived")

logRegModel <- train(logReg, titanicTask)

## 修饰训练器
# WRAP LEARNER ----
logRegWrapper <- makeImputeWrapper("classif.logreg",
                                   cols = list(Age = imputeMean()))
logRegWrapper
```

十倍交叉验证训练模型

```R
# 定义方法
kFold <- makeResampleDesc(method = "RepCV", folds = 10, reps = 50, 
                          stratify = TRUE)

logRegwithImpute <- resample(logRegWrapper, titanicTask, resampling = kFold, 
                             measures = list(acc, fpr, fnr))
logRegwithImpute
###########
Resample Result
Task: imp$data
Learner: classif.logreg.imputed
Aggr perf: acc.test.mean=0.7961500,fpr.test.mean=0.2992605,fnr.test.mean=0.14
44175
Runtime: 10.6986
```

解释模型 优势比odds ratio

为了提取模型参数，必须首先转变我们的mlr模型，logRegModel使用getLearnerModel()函数，然后使用coef()函数，得到log odds

```R
# EXTRACT ODDS RATIOS
logRegModelData <- getLearnerModel(logRegModel)

library(stats)

stats::coef(logRegModelData)

(Intercept) Pclass2 Pclass3 Sexmale Age
3.809661697 -1.000344806 -2.132428850 -2.775928255 -0.038822458
Fare FamSize
0.003218432 -0.243029114
```

log odds转变模型参数至odds rations，同时使用confint()函数计算95%置信区间，帮助我们决定哪个变量是更有证据的（log ） 

```R
exp(cbind(Odds_Ratio = coef(logRegModelData), confint(logRegModelData)))

Waiting for profiling to be done...
Odds_Ratio 2.5 % 97.5 %
(Intercept) 45.13516691 19.14718874 109.72483921
Pclass2 0.36775262 0.20650392 0.65220841
Pclass3 0.11854901 0.06700311 0.20885220
```

这里注意怎么去解释连续型和离散型变量。

95%置信区间表明每个变量具有预测价值的证据的强度。比值比为1表示比值相等，且该变量对预测没有影响。因此，如果95%置信区间包括值1，例如Fare变量，那么这可能表明该变量没有任何贡献。

使用模型去预测未知数据

```R
# USING THE MODEL TO MAKE PREDICTIONS ----
data(titanic_test, package = "titanic")

titanicNew <- as_tibble(titanic_test)

titanicNewClean <- titanicNew %>%
  mutate_at(.vars = c("Sex", "Pclass"), .funs = factor) %>%
  mutate(FamSize = SibSp + Parch) %>%
  select(Pclass, Sex, Age, Fare, FamSize)

predict(logRegModel, newdata = titanicNewClean)

Prediction: 418 observations
predict.type: prob
threshold: 0=0.50,1=0.50
time: 0.00
prob.0 prob.1 response
1 0.9178036 0.08219636 0
2 0.5909570 0.40904305 0
3 0.9123303 0.08766974 0
```



#### 判别分析最大化分类

![image-20220427223932719](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202204272239954.png)

总结判别分析是一种监督学习算法，它将数据投射到低维表示上以创建判别函数。判别函数是原始(连续)变量的线性组合，它最大限度地分离类质心，同时最小化沿着它们的每个类的方差。判别分析有多种形式，其中最基本的是LDA和QDA。LDA学习类之间的线性决策边界，并假设类正态分布，具有相等的协方差。QDA可以学习类之间的弯曲决策边界，并假设每个类都是正态分布的，但不假设协方差相等。判别函数的数量是类的数量减去1，即预测变量的数量的小数。类别预测使用贝叶斯规则来估计属于每个类别的一个案例的后验概率。



**优势**

1. 可以降维一个缩减为更易于管理的数量
2. 可以用于分类，也可以作为预处理降维技术，用于在数据集上表现更会的其他分类算法
3. QDA可以学习不同类别之间的弯曲决策边界（LDA不行）

**劣势**

1. 他们只可以处理连续预测 （虽然重编码分类变量为数值可以解决）
2. 他们假设数据符合正态分布，如果不符合表先不好
3. LDA只可以类别直接线性平面
4. LDA假设类别之间等量的方差，如果不是表现很差
5. QDA相比LDA更灵活，但是更容易过拟合



> Code- from book1



构建模型

```R
lda.fit <- lda(class ~ ., data = train)
lda.fit

lda(class ~ ., data = train)
Prior probabilities of groups:
benign malignant
0.6371308 0.3628692
Group means:
thick u.size u.shape adhsn s.size nucl
chrom
benign 2.9205 1.30463 1.41390 1.32450 2.11589
1.39735 2.08278
malignant 7.1918 6.69767 6.68604 5.66860 5.50000

## 判别评分的直方图 和密度图
plot(lda.fit, type="both")
```

![image-20220507095708773](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202205070957891.png)

LDA模型可以用predict()函数得到3种元素（class、posterior和x）的列表。class元素是对
良性或恶性的预测，posterior是值为x的评分可能属于某个类别的概率，x是线性判别评分。通过下面的函数，我们仅提取恶性观测的概率：

```
train.lda.probs <- predict(lda.fit)$posterior[, 2]
misClassError(trainY, train.lda.probs)
confusionMatrix(trainY, train.lda.probs)
```

在测试集上的表现

```
## 测试集上的表现
test.lda.probs <- predict(lda.fit, newdata = test)$posterior[, 2]
misClassError(testY, test.lda.probs)
confusionMatrix(testY, test.lda.probs)
```

二次判别分析(QDA), 结果中有分组均值，但是没有系数，因为这是二次函数

```R
### 下面用二次判别分析（QDA）模型来拟合数据
qda.fit <- qda(class ~ ., data =train)
qda.fit
## 结果中有分组均值，和LCD一样，但是没有系数，因为这是二次函数

## 训练集上
train.qda.probs <- predict(qda.fit)$posterior[, 2]
misClassError(trainY, train.qda.probs)
confusionMatrix(trainY, train.qda.probs
)

#测试集
test.qda.probs <- predict(qda.fit, newdata = test)$posterior[, 2]
misClassError(testY, test.qda.probs)
confusionMatrix(testY, test.qda.probs)
```

QDA的结果表现的特别差

除了LDA和QDA模型外，介绍一种MARS模型，测试效果可能超过了随机森林和提升树，可以自动进行变量选择，通过交叉验证。

我的设定是使用5折交叉验证来选择模型（pmethod="cv"，nfold = 5），重复3次（ncross = 3）；使用没有交互项的加法模型（degree = 1），每个输入特征只使用一个铰链函数（minspan = -1）。

```

#install.packages('earth')
library(earth)
set.seed(1)
## 参数有效数量 = 输入特征数量 + 惩罚系数*(输入特征数量-1)/2
earth.fit <- earth(class ~ ., data = train,
                   pmethod = "cv",
                   nfold = 5, ## 5折
                   ncross = 3, ## 重复3次
                   degree = 1, # 使用没有交互项的加法模型
                   minspan = -1, # 只使用一个铰链函数
                   glm=list(family=binomial)
                  
                  )
                
## summary(earth.fit)  
plotmo(earth.fit)
plotd(earth.fit)
```

![image-20220507101645085](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202205071016262.png)

变量之间的相对重要性，nsubsets是精简过程完成后包含这个变量的模型的个数，其中这个变量贡献的gcv和rss值的减少量

```R
evimp(earth.fit)
> test.earth.probs <- predict(earth.fit, newdata =
test, type = "response")
> misClassError(testY, test.earth.probs)
[1] 0.0287
> confusionMatrix(testY, test.earth.probs)
0 1
0 138 2
1 4 65
```

**关于模型选择**

从模型中计算出混淆矩阵和错误率，对于分类模型，受试者工作特征（ROC）图是一个很有效的工具，图中Y轴是真阳性率 X轴是假阳性率 AUC是一个衡量分类器性能的有效指标

```R
library(ROCR)
## 受试者工作特征（ROC） Y轴是真阳性率 X轴是假阳性率

bad.fit <- glm(class ~ thick, family = binomial, data = train)
test.bad.probs <- predict(bad.fit, newdata = test, type = "response") #save probabilities

##全模型的ROC图
# 首先建立一个对象，保存对实际分类的预测概率，然后对这个对象建立一个带有TPR和FPR的对象
pred.full <- prediction(test.probs, test$class)
perf.full <- performance(pred.full, "tpr", "fpr")
plot(perf.full, main = "ROC", col = 1)
```

![image-20220507102524054](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202205071025172.png)

完美的分类器 是X轴垂直上升的线，如果模型的预测和随机选择没区别 ，那么形成左下都右上的对角线。现在加入其它的模型

```R
## 加上BIC建立的模型
pred.bic <- prediction(test.bic.probs, test$class)
perf.bic <- performance(pred.bic, "tpr", "fpr")
plot(perf.bic, col = 2, add = TRUE)

## 加入MARS模型
pred.bad <- prediction(test.bad.probs, test$class)
perf.bad <- performance(pred.bad, "tpr", "fpr")
plot(perf.bad, col = 3, add = TRUE)

pred.earth <- prediction(test.earth.probs, test$class)
perf.earth <- performance(pred.earth, "tpr", "fpr")
plot(perf.earth, col = 4, add = TRUE)
legend(0.6, 0.6, c("FULL", "BIC", "BAD", "EARTH"), 1:4)
# 最后计算AUC曲线，使用oerformance对象，将tpr和fpr 换成auc即可
performance(pred.full, "auc")@y.values
performance(pred.bic, "auc")@y.values
performance(pred.bad, "auc")@y.values
performance(pred.earth, "auc")@y.values
```

![image-20220507102836496](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202205071028710.png)

除了糟糕模型外 其它几个模型在预测能力方面没有区别，将训练集合测试集重新随机化，再60/40划分做一遍，若还是相同结果，又归结到权衡的问题。



> code - from book2

```R
# LOAD PACKAGES ----
library(mlr)

library(tidyverse)

# LOAD DATA ----
install.packages("HDclassif")

data(wine, package = "HDclassif")

wineTib <- as_tibble(wine)

wineTib

# CLEAN DATA ----
names(wineTib) <- c("Class", "Alco", "Malic", "Ash", "Alk", "Mag", 
                    "Phe", "Flav", "Non_flav", "Proan", "Col", "Hue", 
                    "OD", "Prol")

wineTib$Class <- as.factor(wineTib$Class)

wineTib
```

绘制各个变量箱线图 观察分布

```
# PLOTTING THE DATA ----
wineUntidy <- gather(wineTib, "Variable", "Value", -Class)

ggplot(wineUntidy, aes(Class, Value)) +
  facet_wrap(~ Variable, scales = "free_y") +
  geom_boxplot() +
  theme_bw()

```

![image-20220507103810648](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202205071038144.png)

构建LDA训练器模型

```
# CREATE TASK AND LEARNER, AND TRAIN MODEL ----
wineTask <- makeClassifTask(data = wineTib, target = "Class")

lda <- makeLearner("classif.lda")

ldaModel <- train(lda, wineTask)
```

提取每个变量的LD值

```R
# EXTRACTING DISCRIMINANT FUNCTION SCORES FOR EACH CASE ----
ldaModelData <- getLearnerModel(ldaModel)

ldaPreds <- predict(ldaModelData)$x

head(ldaPreds)
```

根据LDA值绘图

```
# PLOT DISCRIMINANT FUNCTIONS ----
wineTib %>%
  mutate(LD1 = ldaPreds[, 1], 
         LD2 = ldaPreds[, 2]) %>%
  ggplot(aes(LD1, LD2, col = Class)) +
  geom_point() +
  stat_ellipse() +
  theme_bw()
```

![image-20220507104119775](https://kaikaihe.oss-cn-beijing.aliyuncs.com/image/202205071041913.png)

**QDA模型构建**

```
# MAKE QDA LEARNER AND TRAIN MODEL----
qda <- makeLearner("classif.qda")

qdaModel <- train(qda, wineTask)
```

交叉验证LDA和QDA模型

```R
# CROSS-VALIDATING MODELS ----
kFold <- makeResampleDesc(method = "RepCV", folds = 10, reps = 50, 
                          stratify = TRUE)

ldaCV <- resample(learner = lda, task = wineTask, resampling = kFold,
                    measures = list(mmce, acc))

qdaCV <- resample(learner = qda, task = wineTask, resampling = kFold,
                  measures = list(mmce, acc))
ldaCV$aggr
mmce.test.mean acc.test.mean
0.01177012 0.98822988
qdaCV$aggr
mmce.test.mean acc.test.mean
0.007977296 0.992022704
```

构建混合矩阵查看

```
# CALCULATE CONFUSION MATRICES ----
calculateConfusionMatrix(ldaCV$pred, relative = TRUE)

calculateConfusionMatrix(qdaCV$pred, relative = TRUE)
```

使用QDA模型预测新的变量

```
# USING THE QDA MODEL TO MAKE PREDICTIONS ----
poisoned <- tibble(Alco = 13, Malic = 2, Ash = 2.2, Alk = 19, Mag = 100, 
                   Phe = 2.3, Flav = 2.5, Non_flav = 0.35, Proan = 1.7,
                   Col = 4, Hue = 1.1, OD = 3, Prol = 750)

predict(qdaModel, newdata = poisoned)
```

















#### 朴素贝叶斯和支持向量机分类

朴素贝叶斯可以混合连续和分类变量，而SVM必须转变分类变量为数字形式，朴素贝叶斯更适用于垃圾邮件检测和文字分类，线性平面分割没有SVM表现的好。

朴素贝叶斯的计算能力小于SVM，如果可以用朴素贝叶斯解决你的问题，不必要用贝叶斯解决









#### 决策树分类



#### 随机森林和提升树提高决策能力









### Regression





### Dimension





### Clustering











## 参考资料

[说人话的统计学——终点·起点 | 协和八](https://mp.weixin.qq.com/s/8EJTWTGs_043yzvZINVecQ)

